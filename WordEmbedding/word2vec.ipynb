{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class InputData:\n",
    "    def __init__(self,input_file_name,min_count):\n",
    "        self.input_file_name = input_file_name\n",
    "        self.index = 0\n",
    "        self.input_file = open(self.input_file_name,\"r\",encoding=\"utf-8\")\n",
    "        self.min_count = min_count\n",
    "        self.wordid_frequency_dict = dict()\n",
    "        self.word_count = 0\n",
    "        self.word_count_sum = 0\n",
    "        self.sentence_count = 0\n",
    "        self.id2word_dict = dict()\n",
    "        self.word2id_dict = dict()\n",
    "        self._init_dict()  # 初始化字典\n",
    "        self.sample_table = []\n",
    "        self._init_sample_table()  # 初始化负采样映射表\n",
    "        self.get_wordId_list()\n",
    "        self.word_pairs_queue = deque()\n",
    "        # 结果展示\n",
    "        print('Word Count is:', self.word_count)\n",
    "        print('Word Count Sum is', self.word_count_sum)\n",
    "        print('Sentence Count is:', self.sentence_count)\n",
    "    def _init_dict(self):\n",
    "        word_freq = dict()\n",
    "        for line in self.input_file:\n",
    "            line = line.strip().split()\n",
    "            self.word_count_sum +=len(line)\n",
    "            self.sentence_count +=1\n",
    "            for i,word in enumerate(line):\n",
    "                if i%1000000==0:\n",
    "                    print (i,len(line))\n",
    "                if word_freq.get(word)==None:\n",
    "                    word_freq[word] = 1\n",
    "                else:\n",
    "                    word_freq[word] += 1\n",
    "        for i,word in enumerate(word_freq):\n",
    "            if i % 100000 == 0:\n",
    "                print(i, len(word_freq))\n",
    "            if word_freq[word]<self.min_count:\n",
    "                self.word_count_sum -= word_freq[word]\n",
    "                continue\n",
    "            self.word2id_dict[word] = len(self.word2id_dict)\n",
    "            self.id2word_dict[len(self.id2word_dict)] = word\n",
    "            self.wordid_frequency_dict[len(self.word2id_dict)-1] = word_freq[word]\n",
    "        self.word_count =len(self.word2id_dict)\n",
    "    def _init_sample_table(self):\n",
    "        sample_table_size = 1e8\n",
    "        pow_frequency = np.array(list(self.wordid_frequency_dict.values())) ** 0.75\n",
    "        word_pow_sum = sum(pow_frequency)\n",
    "        ratio_array = pow_frequency / word_pow_sum\n",
    "        word_count_list = np.round(ratio_array * sample_table_size)\n",
    "        for word_index, word_freq in enumerate(word_count_list):\n",
    "            self.sample_table += [word_index] * int(word_freq)\n",
    "        self.sample_table = np.array(self.sample_table)\n",
    "        np.random.shuffle(self.sample_table)\n",
    "    def get_wordId_list(self):\n",
    "        self.input_file = open(self.input_file_name, encoding=\"utf-8\")\n",
    "        sentence = self.input_file.readline()\n",
    "        wordId_list = []  # 一句中的所有word 对应的 id\n",
    "        sentence = sentence.strip().split(' ')\n",
    "        for i,word in enumerate(sentence):\n",
    "            if i%1000000==0:\n",
    "                print (i,len(sentence))\n",
    "            try:\n",
    "                word_id = self.word2id_dict[word]\n",
    "                wordId_list.append(word_id)\n",
    "            except:\n",
    "                continue\n",
    "        self.wordId_list = wordId_list\n",
    "    def get_batch_pairs(self,batch_size,window_size):\n",
    "        while len(self.word_pairs_queue) < batch_size:\n",
    "            for _ in range(1000):\n",
    "                if self.index == len(self.wordId_list):\n",
    "                    self.index = 0\n",
    "                wordId_w = self.wordId_list[self.index]\n",
    "                for i in range(max(self.index - window_size, 0),\n",
    "                                         min(self.index + window_size + 1,len(self.wordId_list))):\n",
    "\n",
    "                    wordId_v = self.wordId_list[i]\n",
    "                    if self.index == i:  # 上下文=中心词 跳过\n",
    "                        continue\n",
    "                    self.word_pairs_queue.append((wordId_w, wordId_v))\n",
    "                self.index+=1\n",
    "        result_pairs = []  # 返回mini-batch大小的正采样对\n",
    "        for _ in range(batch_size):\n",
    "            result_pairs.append(self.word_pairs_queue.popleft())\n",
    "        return result_pairs\n",
    "\n",
    "\n",
    "    # 获取负采样 输入正采样对数组 positive_pairs，以及每个正采样对需要的负采样数 neg_count 从采样表抽取负采样词的id\n",
    "    # （假设数据够大，不考虑负采样=正采样的小概率情况）\n",
    "    def get_negative_sampling(self, positive_pairs, neg_count):\n",
    "        neg_v = np.random.choice(self.sample_table, size=(len(positive_pairs), neg_count)).tolist()\n",
    "        return neg_v\n",
    "\n",
    "    # 估计数据中正采样对数，用于设定batch\n",
    "    def evaluate_pairs_count(self, window_size):\n",
    "        return self.word_count_sum * (2 * window_size) - self.sentence_count * (\n",
    "                    1 + window_size) * window_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.w_embeddings = nn.Embedding(vocab_size,embed_size)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self._init_emb()\n",
    "\n",
    "    def _init_emb(self):\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_w, pos_v, neg_v):\n",
    "        emb_w = self.w_embeddings(torch.LongTensor(pos_w))  # 转为tensor 大小 [ mini_batch_size * emb_dimension ]\n",
    "        emb_v = self.v_embeddings(torch.LongTensor(pos_v))\n",
    "        neg_emb_v = self.v_embeddings(torch.LongTensor(neg_v))  # 转换后大小 [ negative_sampling_number * mini_batch_size * emb_dimension ]\n",
    "        score = torch.mul(emb_w, emb_v)\n",
    "\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = torch.clamp(score, max=10, min=-10)\n",
    "        score = F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.bmm(neg_emb_v, emb_w.unsqueeze(2))\n",
    "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        # L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]\n",
    "        loss = - torch.sum(score) - torch.sum(neg_score)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding_1 = self.w_embeddings.weight.data.cpu().numpy()\n",
    "        embedding_2 = self.v_embeddings.weight.data.cpu().numpy()\n",
    "        embedding = (embedding_1+embedding_2)/2\n",
    "        fout = open(file_name, 'w')\n",
    "        fout.write('%d %d\\n' % (len(id2word), self.embed_size))\n",
    "        for wid, w in id2word.items():\n",
    "            e = embedding[wid]\n",
    "            e = ' '.join(map(lambda x: str(x), e))\n",
    "            fout.write('%s %s\\n' % (w, e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skip_gram_nge_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\WordEmbedding\\word2vec.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/word2vec.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskip_gram_nge_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SkipGramModel\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/word2vec.ipynb#ch0000007?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minput_data\u001b[39;00m \u001b[39mimport\u001b[39;00m InputData\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/word2vec.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skip_gram_nge_model'"
     ]
    }
   ],
   "source": [
    "from skip_gram_nge_model import SkipGramModel\n",
    "from input_data import InputData\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def ArgumentParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', type=str, default=\"skip-gram\", help=\"skip-gram or cbow\")\n",
    "    parser.add_argument(\"--window_size\",type=int,default=3,help=\"window size in word2vec\")\n",
    "    parser.add_argument(\"--batch_size\",type=int,default=256,help=\"batch size during training phase\")\n",
    "    parser.add_argument(\"--min_count\",type=int,default=3,help=\"min count of training word\")\n",
    "    parser.add_argument(\"--embed_dimension\",type=int,default=100,help=\"embedding dimension of word embedding\")\n",
    "    parser.add_argument(\"--learning_rate\",type=float,default=0.02,help=\"learning rate during training phase\")\n",
    "    parser.add_argument(\"--neg_count\",type=int,default=5,help=\"neg count of skip-gram\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = ArgumentParser()\n",
    "\n",
    "WINDOW_SIZE = args.window_size  # 上下文窗口c\n",
    "BATCH_SIZE = args.batch_size  # mini-batch\n",
    "MIN_COUNT = args.min_count  # 需要剔除的 低频词 的频\n",
    "EMB_DIMENSION = args.embed_dimension  # embedding维度\n",
    "LR = args.learning_rate  # 学习率\n",
    "NEG_COUNT = args.neg_count  # 负采样数\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, input_file_name, output_file_name):\n",
    "        self.output_file_name = output_file_name\n",
    "        self.data = InputData(input_file_name, MIN_COUNT)\n",
    "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
    "        self.lr = LR\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"SkipGram Training......\")\n",
    "        pairs_count = self.data.evaluate_pairs_count(WINDOW_SIZE)\n",
    "        print(\"pairs_count\", pairs_count)\n",
    "        batch_count = pairs_count / BATCH_SIZE\n",
    "        print(\"batch_count\", batch_count)\n",
    "        process_bar = tqdm(range(int(batch_count)))\n",
    "        for i in process_bar:\n",
    "            pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
    "            pos_w = [int(pair[0]) for pair in pos_pairs]\n",
    "            pos_v = [int(pair[1]) for pair in pos_pairs]\n",
    "            neg_v = self.data.get_negative_sampling(pos_pairs, NEG_COUNT)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model.forward(pos_w, pos_v, neg_v)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if i * BATCH_SIZE % 100000 == 0:\n",
    "                self.lr = self.lr * (1.0 - 1.0 * i / batch_count)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "        self.model.save_embedding(self.data.id2word_dict, self.output_file_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    w2v = Word2Vec(input_file_name='../data/lxc.txt', output_file_name=\"skip_gram_neg.txt\")\n",
    "    w2v.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, word_id, frequency):\n",
    "        self.word_id = word_id\n",
    "        self.frequency = frequency\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.father = None\n",
    "        self.Huffman_code = []\n",
    "        self.path = []\n",
    "\n",
    "\n",
    "\n",
    "class HuffmanTree:\n",
    "    def __init__(self, wordid_frequency_dict):\n",
    "        self.word_count = len(wordid_frequency_dict)\n",
    "        self.wordid_code = dict()\n",
    "        self.wordid_path = dict()\n",
    "        self.root = None\n",
    "        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()]\n",
    "        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()]\n",
    "        print(\"Building huffman tree...\")\n",
    "        self.build_tree(unmerge_node_list)\n",
    "        print(\"Building tree finished\")\n",
    "        # 生成huffman code\n",
    "        print(\"Generating huffman path...\")\n",
    "        self.generate_huffman_code_and_path()\n",
    "        print(\"Generating huffman path finished\")\n",
    "\n",
    "    def merge_node(self, node1, node2):\n",
    "        sum_frequency = node1.frequency + node2.frequency\n",
    "        mid_node_id = len(self.huffman)\n",
    "        father_node = HuffmanNode(mid_node_id, sum_frequency)\n",
    "        if node1.frequency >= node2.frequency:\n",
    "            father_node.left_child = node1\n",
    "            father_node.right_child = node2\n",
    "        else:\n",
    "            father_node.left_child = node2\n",
    "            father_node.right_child = node1\n",
    "        self.huffman.append(father_node)\n",
    "        return father_node\n",
    "\n",
    "    def build_tree(self, node_list):\n",
    "\n",
    "        while len(node_list) > 1:\n",
    "            node_list = sorted(node_list, key=lambda x: x.frequency)\n",
    "            i1 = node_list[0]\n",
    "            i2 = node_list[1]\n",
    "            node_list.remove(i1)\n",
    "            node_list.remove(i2)\n",
    "            father_node = self.merge_node(i1, i2)  # 合并最小的两个节点\n",
    "            node_list.append(father_node)  # 插入新节点\n",
    "\n",
    "        self.root = node_list[0]\n",
    "\n",
    "    def generate_huffman_code_and_path(self):\n",
    "        stack = [self.root]\n",
    "        while len(stack) > 0:\n",
    "            node = stack.pop()\n",
    "            # 顺着左子树走\n",
    "            while node.left_child or node.right_child:\n",
    "                code = node.Huffman_code\n",
    "                path = node.path\n",
    "                node.left_child.Huffman_code = code + [1]\n",
    "                node.right_child.Huffman_code = code + [0]\n",
    "                node.left_child.path = path + [node.word_id]\n",
    "                node.right_child.path = path + [node.word_id]\n",
    "                # 把没走过的右子树加入栈\n",
    "                stack.append(node.right_child)\n",
    "                node = node.left_child\n",
    "            word_id = node.word_id\n",
    "            word_code = node.Huffman_code\n",
    "            word_path = node.path\n",
    "            self.huffman[word_id].Huffman_code = word_code\n",
    "            self.huffman[word_id].path = word_path\n",
    "            # 把节点计算得到的霍夫曼码、路径  写入词典的数值中\n",
    "            self.wordid_code[word_id] = word_code\n",
    "            self.wordid_path[word_id] = word_path\n",
    "\n",
    "    # 获取所有词的正向节点id和负向节点id数组\n",
    "    def get_all_pos_and_neg_path(self):\n",
    "        positive = []  # 所有词的正向路径数组\n",
    "        negative = []  # 所有词的负向路径数组\n",
    "        for word_id in range(self.word_count):\n",
    "            pos_id = []  # 存放一个词 路径中的正向节点id\n",
    "            neg_id = []  # 存放一个词 路径中的负向节点id\n",
    "            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n",
    "                if code == 1:\n",
    "                    pos_id.append(self.huffman[word_id].path[i])\n",
    "                else:\n",
    "                    neg_id.append(self.huffman[word_id].path[i])\n",
    "            positive.append(pos_id)\n",
    "            negative.append(neg_id)\n",
    "        return positive, negative\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    word_frequency = {0: 7, 1: 8, 2: 3, 3: 2, 4: 2}\n",
    "    print(word_frequency)\n",
    "    tree = HuffmanTree(word_frequency)\n",
    "    print(tree.wordid_code)\n",
    "    print(tree.wordid_path)\n",
    "    for i in range(len(word_frequency)):\n",
    "        print(tree.huffman[i].path)\n",
    "    print(tree.get_all_pos_and_neg_path())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(u'已经找到%s篇文章'%nb_sentence)? (2295648212.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print u'已经找到%s篇文章'%nb_sentence\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(u'已经找到%s篇文章'%nb_sentence)?\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "#Keras版的Word2Vec，作者：苏剑林，http://kexue.fm\n",
    "#Keras 2.0.6 ＋ Tensorflow 测试通过\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input,Embedding,Lambda\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "word_size = 128 #词向量维度\n",
    "window = 5 #窗口大小\n",
    "nb_negative = 16 #随机负采样的样本数\n",
    "min_count = 10 #频数少于min_count的词将会被抛弃\n",
    "nb_worker = 4 #读取数据的并发数\n",
    "nb_epoch = 2 #迭代次数，由于使用了adam，迭代次数1～2次效果就相当不错\n",
    "subsample_t = 1e-5 #词频大于subsample_t的词语，会被降采样，这是提高速度和词向量质量的有效方案\n",
    "nb_sentence_per_batch = 20\n",
    "#目前是以句子为单位作为batch，多少个句子作为一个batch（这样才容易估计训练过程中的steps参数，另外注意，样本数是正比于字数的。）\n",
    "\n",
    "import pymongo\n",
    "class Sentences: #语料生成器，必须这样写才是可重复使用的\n",
    "    def __init__(self):\n",
    "        self.db = pymongo.MongoClient().weixin.text_articles\n",
    "    def __iter__(self):\n",
    "        for t in self.db.find(no_cursor_timeout=True).limit(100000):\n",
    "            yield t['words'] #返回分词后的结果\n",
    "\n",
    "sentences = Sentences()\n",
    "words = {} #词频表\n",
    "nb_sentence = 0 #总句子数\n",
    "total = 0. #总词频\n",
    "\n",
    "for d in sentences:\n",
    "    nb_sentence += 1\n",
    "    for w in d:\n",
    "        if w not in words:\n",
    "            words[w] = 0\n",
    "        words[w] += 1\n",
    "        total += 1\n",
    "    if nb_sentence % 10000 == 0:\n",
    "        print u'已经找到%s篇文章'%nb_sentence\n",
    "\n",
    "words = {i:j for i,j in words.items() if j >= min_count} #截断词频\n",
    "id2word = {i+1:j for i,j in enumerate(words)} #id到词语的映射，0表示UNK\n",
    "word2id = {j:i for i,j in id2word.items()} #词语到id的映射\n",
    "nb_word = len(words)+1 #总词数（算上填充符号0）\n",
    "\n",
    "subsamples = {i:j/total for i,j in words.items() if j/total > subsample_t}\n",
    "subsamples = {i:subsample_t/j+(subsample_t/j)**0.5 for i,j in subsamples.items()} #这个降采样公式，是按照word2vec的源码来的\n",
    "subsamples = {word2id[i]:j for i,j in subsamples.items() if j < 1.} #降采样表\n",
    "\n",
    "def data_generator(): #训练数据生成器\n",
    "    while True:\n",
    "        x,y = [],[]\n",
    "        _ = 0\n",
    "        for d in sentences:\n",
    "            d = [0]*window + [word2id[w] for w in d if w in word2id] + [0]*window\n",
    "            r = np.random.random(len(d))\n",
    "            for i in range(window, len(d)-window):\n",
    "                if d[i] in subsamples and r[i] > subsamples[d[i]]: #满足降采样条件的直接跳过\n",
    "                    continue\n",
    "                x.append(d[i-window:i]+d[i+1:i+1+window])\n",
    "                y.append([d[i]])\n",
    "            _ += 1\n",
    "            if _ == nb_sentence_per_batch:\n",
    "                x,y = np.array(x),np.array(y)\n",
    "                z = np.zeros((len(x), 1))\n",
    "                yield [x,y],z\n",
    "                x,y = [],[]\n",
    "                _ = 0\n",
    "\n",
    "#CBOW输入\n",
    "input_words = Input(shape=(window*2,), dtype='int32')\n",
    "input_vecs = Embedding(nb_word, word_size, name='word2vec')(input_words)\n",
    "input_vecs_sum = Lambda(lambda x: K.sum(x, axis=1))(input_vecs) #CBOW模型，直接将上下文词向量求和\n",
    "\n",
    "#构造随机负样本，与目标组成抽样\n",
    "target_word = Input(shape=(1,), dtype='int32')\n",
    "negatives = Lambda(lambda x: K.random_uniform((K.shape(x)[0], nb_negative), 0, nb_word, 'int32'))(target_word)\n",
    "samples = Lambda(lambda x: K.concatenate(x))([target_word,negatives]) #构造抽样，负样本随机抽。负样本也可能抽到正样本，但概率小。\n",
    "\n",
    "#只在抽样内做Dense和softmax\n",
    "softmax_weights = Embedding(nb_word, word_size, name='W')(samples)\n",
    "softmax_biases = Embedding(nb_word, 1, name='b')(samples)\n",
    "softmax = Lambda(lambda x: \n",
    "                    K.softmax((K.batch_dot(x[0], K.expand_dims(x[1],2))+x[2])[:,:,0])\n",
    "                )([softmax_weights,input_vecs_sum,softmax_biases]) #用Embedding层存参数，用K后端实现矩阵乘法，以此复现Dense层的功能\n",
    "\n",
    "#留意到，我们构造抽样时，把目标放在了第一位，也就是说，softmax的目标id总是0，这可以从data_generator中的z变量的写法可以看出\n",
    "\n",
    "model = Model(inputs=[input_words,target_word], outputs=softmax)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#请留意用的是sparse_categorical_crossentropy而不是categorical_crossentropy\n",
    "\n",
    "model.fit_generator(data_generator(), \n",
    "                    steps_per_epoch=nb_sentence/nb_sentence_per_batch, \n",
    "                    epochs=nb_epoch,\n",
    "                    workers=nb_worker,\n",
    "                    use_multiprocessing=True\n",
    "                   )\n",
    "\n",
    "model.save_weights('word2vec.model')\n",
    "\n",
    "#通过词语相似度，检查我们的词向量是不是靠谱的\n",
    "embeddings = model.get_weights()[0]\n",
    "normalized_embeddings = embeddings / (embeddings**2).sum(axis=1).reshape((-1,1))**0.5\n",
    "\n",
    "def most_similar(w):\n",
    "    v = normalized_embeddings[word2id[w]]\n",
    "    sims = np.dot(normalized_embeddings, v)\n",
    "    sort = sims.argsort()[::-1]\n",
    "    sort = sort[sort > 0]\n",
    "    return [(id2word[i],sims[i]) for i in sort[:10]]\n",
    "\n",
    "import pandas as pd\n",
    "pd.Series(most_similar(u'科学'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
