{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\WordEmbedding\\glove.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m Vectors\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimalarity\u001b[39m(word1, word2):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "import numpy as np\n",
    "\n",
    "def simalarity(word1, word2):\n",
    "    GloVectors = Vectors(name='./glove.6B/glove.6B.300d.txt')\n",
    "    word1_vec = GloVectors.vectors[GloVectors.stoi[word1]].numpy()\n",
    "    word2_vec = GloVectors.vectors[GloVectors.stoi[word2]].numpy()\n",
    "    dot = np.dot(word1_vec.T, word2_vec)\n",
    "    return dot / np.sqrt(np.sum(word1_vec ** 2)) / np.sqrt(\n",
    "        np.sum(word2_vec ** 2))\n",
    "\n",
    "print(simalarity('well', 'good')) #0.7045711\n",
    "print(simalarity('bad', 'good')) #0.64452195\n",
    "print(simalarity('normal', 'good')) #0.41142386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import time\n",
    "from gensim import corpora\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(2021) #设置固定的随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randmatrix(m, n):\n",
    "    \"\"\"Creates an m x n matrix of random values drawn using\n",
    "    the Xavier Glorot method.\"\"\"\n",
    "    val = np.sqrt(6.0 / (m + n))\n",
    "    return np.random.uniform(-val, val, size=(m, n))\n",
    "\n",
    "\n",
    "def log_of_array_ignoring_zeros(M):\n",
    "    log_M = M.copy()\n",
    "    mask = log_M > 0\n",
    "    log_M[mask] = np.log(log_M[mask])\n",
    "    return log_M\n",
    "\n",
    "\n",
    "def noise(n, scale=0.01):\n",
    "    return np.random.normal(0, scale, size=n)\n",
    "\n",
    "\n",
    "class AdaGradOptimizer:\n",
    "    def __init__(self, learning_rate, initial_accumulator_value=0.1,momentum=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_accumulator_value = initial_accumulator_value\n",
    "        self._momentum = momentum\n",
    "\n",
    "    def get_step(self, grad):\n",
    "        if self._momentum is None:\n",
    "            self._momentum = self.initial_accumulator_value * np.ones_like(grad)\n",
    "        self._momentum += grad ** 2\n",
    "        return self.learning_rate * grad / np.sqrt(self._momentum)\n",
    "\n",
    "\n",
    "class GloVe(object):\n",
    "    def __init__(self, n, max_iter, learning_rate):\n",
    "        self.n = n\n",
    "        self.max_iter = max_iter\n",
    "        self.xmax = 100\n",
    "        self.alpha = 0.75\n",
    "        self.mittens = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = 1e-4\n",
    "        self.display_progress = 100\n",
    "        self.model = None\n",
    "        self.n_words = None\n",
    "        self.log_dir = None\n",
    "        self.log_subdir = None\n",
    "        self.errors = list()\n",
    "        self.test_mode = False\n",
    "\n",
    "    def _initialize(self, coincidence):\n",
    "        self.n_words = coincidence.shape[0]\n",
    "        bounded = np.minimum(coincidence, self.xmax)\n",
    "        weights = (bounded / float(self.xmax)) ** self.alpha\n",
    "        log_coincidence = log_of_array_ignoring_zeros(coincidence)\n",
    "        return weights, log_coincidence\n",
    "\n",
    "    def fit(self, X, vocab=None, initial_embedding_dict=None):\n",
    "        weights, log_coincidence = self._initialize(X)\n",
    "        self._initialize_w_c_b(self.n_words, vocab, initial_embedding_dict)\n",
    "        m_loop = tqdm(range(self.max_iter))\n",
    "        for iteration in m_loop:\n",
    "            pred = self._make_prediction()\n",
    "            gradients, error = self._get_gradients_and_error(\n",
    "                pred, log_coincidence, weights)\n",
    "            self.errors.append(error)\n",
    "            self._apply_updates(gradients)\n",
    "            m_loop.set_description(\"Iteration {}:error {:4.4f}\".format(iteration + 1, error))\n",
    "        return self.W + self.C\n",
    "\n",
    "    def _check_shapes(self, gradients):\n",
    "        assert gradients['W'].shape == self.W.shape\n",
    "        assert gradients['C'].shape == self.C.shape\n",
    "        assert gradients['bw'].shape == self.bw.shape\n",
    "        assert gradients['bc'].shape == self.bc.shape\n",
    "\n",
    "    def _initialize_w_c_b(self, n_words, vocab, initial_embedding_dict):\n",
    "        self.W = randmatrix(n_words, self.n)  # Word weights.\n",
    "        self.C = randmatrix(n_words, self.n)  # Context weights.\n",
    "        if initial_embedding_dict:\n",
    "            assert self.n == len(next(iter(initial_embedding_dict.values())))\n",
    "\n",
    "            self.original_embedding = np.zeros((len(vocab), self.n))\n",
    "            self.has_embedding = np.zeros(len(vocab), dtype=bool)\n",
    "\n",
    "            for i, w in enumerate(vocab):\n",
    "                if w in initial_embedding_dict:\n",
    "                    self.has_embedding[i] = 1\n",
    "                    embedding = np.array(initial_embedding_dict[w])\n",
    "                    self.original_embedding[i] = embedding\n",
    "                    # Divide the original embedding into W and C,\n",
    "                    # plus some noise to break the symmetry that would\n",
    "                    # otherwise cause both gradient updates to be\n",
    "                    # identical.\n",
    "                    self.W[i] = 0.5 * embedding + noise(self.n)\n",
    "                    self.C[i] = 0.5 * embedding + noise(self.n)\n",
    "            # This is for testing. It differs from\n",
    "            # `self.original_embedding` only in that it includes the\n",
    "            # random noise we added above to break the symmetry.\n",
    "            self.G_start = self.W + self.C\n",
    "\n",
    "        self.bw = randmatrix(n_words, 1)\n",
    "        self.bc = randmatrix(n_words, 1)\n",
    "        self.ones = np.ones((n_words, 1))\n",
    "\n",
    "    def _make_prediction(self):\n",
    "        # Here we make use of numpy's broadcasting rules\n",
    "        pred = np.dot(self.W, self.C.T) + self.bw + self.bc.T\n",
    "        return pred\n",
    "\n",
    "    def _get_gradients_and_error(self,\n",
    "                                 predictions,\n",
    "                                 log_coincidence,\n",
    "                                 weights):\n",
    "        # First we compute the GloVe gradients\n",
    "        diffs = predictions - log_coincidence\n",
    "        weighted_diffs = np.multiply(weights, diffs)\n",
    "        wgrad = weighted_diffs.dot(self.C)\n",
    "        cgrad = weighted_diffs.T.dot(self.W)\n",
    "        bwgrad = weighted_diffs.sum(axis=1).reshape(-1, 1)\n",
    "        bcgrad = weighted_diffs.sum(axis=0).reshape(-1, 1)\n",
    "        error = (0.5 * np.multiply(weights, diffs ** 2)).sum()\n",
    "\n",
    "        # Then we add the Mittens term (only if mittens > 0)\n",
    "        if self.mittens > 0:\n",
    "            curr_embedding = self.W + self.C\n",
    "            distance = curr_embedding[self.has_embedding, :] - \\\n",
    "                       self.original_embedding[self.has_embedding, :]\n",
    "            wgrad[self.has_embedding, :] += 2 * self.mittens * distance\n",
    "            cgrad[self.has_embedding, :] += 2 * self.mittens * distance\n",
    "            error += self.mittens * (\n",
    "                    np.linalg.norm(distance, ord=2, axis=1) ** 2).sum()\n",
    "        return {'W': wgrad, 'C': cgrad, 'bw': bwgrad, 'bc': bcgrad}, error\n",
    "\n",
    "    def _apply_updates(self, gradients):\n",
    "      \n",
    "        if not hasattr(self, 'optimizers'):\n",
    "            self.optimizers = \\\n",
    "                {obj: AdaGradOptimizer(self.learning_rate)\n",
    "                 for obj in ['W', 'C', 'bw', 'bc']}\n",
    "        self.W -= self.optimizers['W'].get_step(gradients['W'])\n",
    "        self.C -= self.optimizers['C'].get_step(gradients['C'])\n",
    "        self.bw -= self.optimizers['bw'].get_step(gradients['bw'])\n",
    "        self.bc -= self.optimizers['bc'].get_step(gradients['bc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'doc.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\WordEmbedding\\glove.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000006?line=30'>31</a>\u001b[0m \u001b[39m#模型训练\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000006?line=32'>33</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000006?line=33'>34</a>\u001b[0m     \u001b[39m# 生成词汇相关矩阵\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000006?line=34'>35</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mdoc.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000006?line=35'>36</a>\u001b[0m         sentences \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/glove.ipynb#ch0000006?line=37'>38</a>\u001b[0m     texts \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'doc.txt'"
     ]
    }
   ],
   "source": [
    "def leftRight(c_pos, max_len, window):\n",
    "    return c_pos - window if c_pos - window > 0 else 0, \\\n",
    "           c_pos + window + 1 if c_pos + window + 1 < max_len else max_len\n",
    "\n",
    "\n",
    "def getCoMatriex(texts, token_id, window=2):\n",
    "    n_matrix = len(token_id)\n",
    "    word_matrix = np.zeros(shape=[n_matrix, n_matrix])\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        k = len(texts[i])\n",
    "        for j in range(k):\n",
    "            left, right = leftRight(j, k, window)\n",
    "            c_word = texts[i][j]\n",
    "            c_pos = token_id[c_word]\n",
    "            for m in range(left, right):\n",
    "                # 计算共现矩阵\n",
    "                t_word = texts[i][m]\n",
    "                t_pos = token_id[t_word]\n",
    "                if m != j and t_word != c_word:\n",
    "                    word_matrix[c_pos][t_pos] += 1\n",
    "    return word_matrix\n",
    "\n",
    "\n",
    "\n",
    "def getCorpora(texts):\n",
    "    dct = corpora.Dictionary(texts)\n",
    "    token2idDict = dct.token2id\n",
    "    return dct, token2idDict\n",
    "\n",
    "#模型训练\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成词汇相关矩阵\n",
    "    with open(\"doc.txt\", \"r\", encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    texts = []\n",
    "    for text in sentences:\n",
    "        texts.append(jieba.lcut(text))\n",
    "    n_dims = 10\n",
    "\n",
    "     # 获得语料字典\n",
    "    dct, token_id = getCorpora(texts) \n",
    "\n",
    "    stratTime = time.time()\n",
    "    # 计算共现矩阵\n",
    "    wordComatrix = getCoMatriex(texts, token_id, window=2)\n",
    "    print(\"total time cost:\", time.time() - stratTime)\n",
    "\n",
    "    # print(word_matrix)\n",
    "    # 设置GloVe模型\n",
    "    glove = GloVe(n=n_dims, max_iter=6000, learning_rate=0.004)\n",
    "    #获得GloVe模型的词向量\n",
    "    wordEmbedding = glove.fit(wordComatrix)\n",
    "    print(wordEmbedding.shape)\n",
    "    # 查询词向量\n",
    "    print(\"jieba的词向量为:\", wordEmbedding[token_id['jieba']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
