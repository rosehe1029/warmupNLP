{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\WordEmbedding\\fasttext.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mfastTextModel\u001b[39;00m(BaseModel):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39m    A simple implementation of fasttext for text classification\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=4'>5</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, sequence_length, num_classes, vocab_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=5'>6</a>\u001b[0m                  embedding_size, learning_rate, decay_steps, decay_rate,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=6'>7</a>\u001b[0m                  l2_reg_lambda, is_training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000003?line=7'>8</a>\u001b[0m                  initializer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mrandom_normal_initializer(stddev\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class fastTextModel(BaseModel):\n",
    "    \"\"\"\n",
    "    A simple implementation of fasttext for text classification\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "                 embedding_size, learning_rate, decay_steps, decay_rate,\n",
    "                 l2_reg_lambda, is_training=True,\n",
    "                 initializer=tf.random_normal_initializer(stddev=0.1)):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.is_training = is_training\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.initializer = initializer\n",
    " \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y')\n",
    " \n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.instantiate_weight()\n",
    "        self.logits = self.inference()\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    " \n",
    "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
    "        correct_prediction = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'), name='accuracy')\n",
    " \n",
    "    def instantiate_weight(self):\n",
    "        with tf.name_scope('weights'):\n",
    "            self.Embedding = tf.get_variable('Embedding', shape=[self.vocab_size, self.embedding_size],\n",
    "                                             initializer=self.initializer)\n",
    "            self.W_projection = tf.get_variable('W_projection', shape=[self.embedding_size, self.num_classes],\n",
    "                                                initializer=self.initializer)\n",
    "            self.b_projection = tf.get_variable('b_projection', shape=[self.num_classes])\n",
    " \n",
    " \n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        1. word embedding\n",
    "        2. average embedding\n",
    "        3. linear classifier\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # embedding layer\n",
    "        with tf.name_scope('embedding'):\n",
    "            words_embedding = tf.nn.embedding_lookup(self.Embedding, self.input_x)\n",
    "            self.average_embedding = tf.reduce_mean(words_embedding, axis=1)\n",
    " \n",
    "        logits = tf.matmul(self.average_embedding, self.W_projection) +self.b_projection\n",
    " \n",
    "        return logits\n",
    " \n",
    " \n",
    "    def loss(self):\n",
    "        # loss\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "            data_loss = tf.reduce_mean(losses)\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
    "                                if 'bias' not in cand_var.name]) * self.l2_reg_lambda\n",
    "            data_loss += l2_loss * self.l2_reg_lambda\n",
    "            return data_loss\n",
    " \n",
    "    def train(self):\n",
    "        with tf.name_scope('train'):\n",
    "            learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "                                                       self.decay_steps, self.decay_rate,\n",
    "                                                       staircase=True)\n",
    " \n",
    "            train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
    "                                                      learning_rate=learning_rate, optimizer='Adam')\n",
    " \n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\WordEmbedding\\fasttext.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 130>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000004?line=127'>128</a>\u001b[0m     train(x_train, y_train, vocab_processor, x_dev, y_dev)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000004?line=129'>130</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/WordEmbedding/fasttext.ipynb#ch0000004?line=130'>131</a>\u001b[0m     tf\u001b[39m.\u001b[39mapp\u001b[39m.\u001b[39mrun()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def prepocess():\n",
    "    \"\"\"\n",
    "    For load and process data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_process.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    # bulid vocabulary\n",
    "    max_document_length = max(len(x.split(' ')) for x in x_text)\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    " \n",
    "    # shuffle\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    " \n",
    "    # split train/test dataset\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    " \n",
    "    print('Vocabulary Size: {:d}'.format(len(vocab_processor.vocabulary_)))\n",
    "    print('Train/Dev split: {:d}/{:d}'.format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev\n",
    " \n",
    " \n",
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            # allows TensorFlow to fall back on a device with a certain operation implemented\n",
    "            allow_soft_placement= FLAGS.allow_soft_placement,\n",
    "            # allows TensorFlow log on which devices (CPU or GPU) it places operations\n",
    "            log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            # initialize cnn\n",
    "            fasttext = fastTextModel(sequence_length=x_train.shape[1],\n",
    "                      num_classes=y_train.shape[1],\n",
    "                      vocab_size=len(vocab_processor.vocabulary_),\n",
    "                      embedding_size=FLAGS.embedding_size,\n",
    "                      l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                      is_training=True,\n",
    "                      learning_rate=FLAGS.learning_rate,\n",
    "                      decay_steps=FLAGS.decay_steps,\n",
    "                      decay_rate=FLAGS.decay_rate\n",
    "                    )\n",
    " \n",
    "            # output dir for models and summaries\n",
    "            timestamp = str(time.time())\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, 'run', timestamp))\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            print('Writing to {} \\n'.format(out_dir))\n",
    " \n",
    "            # checkpoint dir. checkpointing – saving the parameters of your model to restore them later on.\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, FLAGS.ckpt_dir))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, 'model')\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    " \n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, 'vocab'))\n",
    " \n",
    "            # Initialize all\n",
    "            sess.run(tf.global_variables_initializer())\n",
    " \n",
    " \n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                :param x_batch:\n",
    "                :param y_batch:\n",
    "                :return:\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    fasttext.input_x: x_batch,\n",
    "                    fasttext.input_y: y_batch,\n",
    "                }\n",
    "                _, step, loss, accuracy = sess.run(\n",
    "                    [fasttext.train_op, fasttext.global_step, fasttext.loss_val, fasttext.accuracy],\n",
    "                    feed_dict=feed_dict\n",
    "                )\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    " \n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluate model on a dev set\n",
    "                Disable dropout\n",
    "                :param x_batch:\n",
    "                :param y_batch:\n",
    "                :param writer:\n",
    "                :return:\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    fasttext.input_x: x_batch,\n",
    "                    fasttext.input_y: y_batch,\n",
    "                }\n",
    "                step, loss, accuracy = sess.run(\n",
    "                    [fasttext.global_step, fasttext.loss_val, fasttext.accuracy],\n",
    "                    feed_dict=feed_dict\n",
    "                )\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"dev results:{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    " \n",
    "            # generate batches\n",
    "            batches = data_process.batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # training loop\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, fasttext.global_step)\n",
    "                if current_step % FLAGS.validate_every == 0:\n",
    "                    print('\\n Evaluation:')\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print('')\n",
    " \n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print('Save model checkpoint to {} \\n'.format(path))\n",
    " \n",
    "def main(argv=None):\n",
    "    x_train, y_train, vocab_processor, x_dev, y_dev = prepocess()\n",
    "    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
