{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU（Gate Recurrent Unit）是循环神经网络的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_t= \\sigma (w_r \\cdot [h_{t-1},x_t]  )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_t=\\sigma (w_z \\cdot [h_{t-1},x_t]  )$\n",
    "\n",
    "$\\widetilde{h_t}= tanh (w_{\\widetilde{h_t}} \\cdot [r_t * h_{t-1},x_t]  )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ h_t=(1-z_t) *h_{t-1} + z_t * \\widetilde{h_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data_x.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\RNNLSTMGRU\\GRU相关.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/GRU%E7%9B%B8%E5%85%B3.ipynb#ch0000005?line=46'>47</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNo GPU available, training on CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/GRU%E7%9B%B8%E5%85%B3.ipynb#ch0000005?line=48'>49</a>\u001b[0m \u001b[39m# 数据读取&类型转换\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/GRU%E7%9B%B8%E5%85%B3.ipynb#ch0000005?line=49'>50</a>\u001b[0m data_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mData_x.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m))\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/GRU%E7%9B%B8%E5%85%B3.ipynb#ch0000005?line=50'>51</a>\u001b[0m data_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mData_y.csv\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/GRU%E7%9B%B8%E5%85%B3.ipynb#ch0000005?line=52'>53</a>\u001b[0m \u001b[39m# 数据集分割\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:610\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    605\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    606\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m    607\u001b[0m )\n\u001b[0;32m    608\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 610\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:462\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    459\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    461\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    464\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:819\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m    817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 819\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1050\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1047\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown engine: \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m (valid options are \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1048\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m \u001b[39m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m \u001b[39mreturn\u001b[39;00m mapping[engine](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1867\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1864\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[0;32m   1866\u001b[0m \u001b[39m# open handles\u001b[39;00m\n\u001b[1;32m-> 1867\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_handles(src, kwds)\n\u001b[0;32m   1868\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1362\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_handles\u001b[39m(\u001b[39mself\u001b[39m, src: FilePathOrBuffer, kwds: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1359\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \u001b[39m    Let the readers open IOHanldes after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1363\u001b[0m         src,\n\u001b[0;32m   1364\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1365\u001b[0m         encoding\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1366\u001b[0m         compression\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1367\u001b[0m         memory_map\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1368\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1369\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\io\\common.py:642\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m         errors \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    641\u001b[0m     \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    643\u001b[0m         handle,\n\u001b[0;32m    644\u001b[0m         ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    645\u001b[0m         encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    646\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    647\u001b[0m         newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    648\u001b[0m     )\n\u001b[0;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_x.csv'"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import csv\n",
    " \n",
    "# Define LSTM Neural Networks\n",
    "class GruRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, input_size, hidden_size=1, output_size=1, num_layers=1):\n",
    "        super().__init__()\n",
    " \n",
    "        self.lstm = nn.GRU(input_size, hidden_size, num_layers)  # utilize the GRU model in torch.nn\n",
    "        self.linear1 = nn.Linear(hidden_size, 16) # 全连接层\n",
    "        self.linear2 = nn.Linear(16, output_size) # 全连接层\n",
    " \n",
    "    def forward(self, _x):\n",
    "        x, _ = self.gru(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x.view(s * b, h)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = x.view(s, b, -1)\n",
    "        return x\n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "# checking if GPU is available\n",
    "    device = torch.device(\"cpu\")\n",
    " \n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    " \n",
    "    # 数据读取&类型转换\n",
    "    data_x = np.array(pd.read_csv('Data_x.csv', header=None)).astype('float32')\n",
    "    data_y = np.array(pd.read_csv('Data_y.csv', header=None)).astype('float32')\n",
    " \n",
    "    # 数据集分割\n",
    "    data_len = len(data_x)\n",
    "    t = np.linspace(0, data_len, data_len)\n",
    " \n",
    "    train_data_ratio = 0.8  # Choose 80% of the data for training\n",
    "    train_data_len = int(data_len * train_data_ratio)\n",
    " \n",
    "    train_x = data_x[5:train_data_len]\n",
    "    train_y = data_y[5:train_data_len]\n",
    "    t_for_training = t[5:train_data_len]\n",
    " \n",
    "    test_x = data_x[train_data_len:]\n",
    "    test_y = data_y[train_data_len:]\n",
    "    t_for_testing = t[train_data_len:]\n",
    " \n",
    "    # ----------------- train -------------------\n",
    "    INPUT_FEATURES_NUM = 5\n",
    "    OUTPUT_FEATURES_NUM = 1\n",
    "    train_x_tensor = train_x.reshape(-1, 1, INPUT_FEATURES_NUM)  # set batch size to 1\n",
    "    train_y_tensor = train_y.reshape(-1, 1, OUTPUT_FEATURES_NUM)  # set batch size to 1\n",
    " \n",
    "    # transfer data to pytorch tensor\n",
    "    train_x_tensor = torch.from_numpy(train_x_tensor)\n",
    "    train_y_tensor = torch.from_numpy(train_y_tensor)\n",
    " \n",
    "    gru_model = GruRNN(INPUT_FEATURES_NUM, 30, output_size=OUTPUT_FEATURES_NUM, num_layers=1)  # 30 hidden units\n",
    "    print('GRU model:', gru_model)\n",
    "    print('model.parameters:', gru_model.parameters)\n",
    "    print('train x tensor dimension:', Variable(train_x_tensor).size())\n",
    " \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(gru_model.parameters(), lr=1e-2)\n",
    " \n",
    "    prev_loss = 1000\n",
    "    max_epochs = 2000\n",
    " \n",
    "    train_x_tensor = train_x_tensor.to(device)\n",
    " \n",
    "    for epoch in range(max_epochs):\n",
    "        output = gru_model(train_x_tensor).to(device)\n",
    "        loss = criterion(output, train_y_tensor)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        if loss < prev_loss:\n",
    "            torch.save(gru_model.state_dict(), 'lstm_model.pt')  # save model parameters to files\n",
    "            prev_loss = loss\n",
    " \n",
    "        if loss.item() < 1e-4:\n",
    "            print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch + 1, max_epochs, loss.item()))\n",
    "            print(\"The loss value is reached\")\n",
    "            break\n",
    "        elif (epoch + 1) % 100 == 0:\n",
    "            print('Epoch: [{}/{}], Loss:{:.5f}'.format(epoch + 1, max_epochs, loss.item()))\n",
    " \n",
    "    # prediction on training dataset\n",
    "    pred_y_for_train = gru_model(train_x_tensor).to(device)\n",
    "    pred_y_for_train = pred_y_for_train.view(-1, OUTPUT_FEATURES_NUM).data.numpy()\n",
    " \n",
    "    # ----------------- test -------------------\n",
    "    lstm_model = gru_model .eval()  # switch to testing model\n",
    " \n",
    "    # prediction on test dataset\n",
    "    test_x_tensor = test_x.reshape(-1, 1,\n",
    "                                   INPUT_FEATURES_NUM)\n",
    "    test_x_tensor = torch.from_numpy(test_x_tensor)  # 变为tensor\n",
    "    test_x_tensor = test_x_tensor.to(device)\n",
    " \n",
    "    pred_y_for_test = gru_model(test_x_tensor).to(device)\n",
    "    pred_y_for_test = pred_y_for_test.view(-1, OUTPUT_FEATURES_NUM).data.numpy()\n",
    " \n",
    "    loss = criterion(torch.from_numpy(pred_y_for_test), torch.from_numpy(test_y))\n",
    "    print(\"test loss：\", loss.item())\n",
    " \n",
    "    # ----------------- plot -------------------\n",
    "    plt.figure()\n",
    "    plt.plot(t_for_training, train_y, 'b', label='y_trn')\n",
    "    plt.plot(t_for_training, pred_y_for_train, 'y--', label='pre_trn')\n",
    " \n",
    "    plt.plot(t_for_testing, test_y, 'k', label='y_tst')\n",
    "    plt.plot(t_for_testing, pred_y_for_test, 'm--', label='pre_tst')\n",
    " \n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('Vce')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
