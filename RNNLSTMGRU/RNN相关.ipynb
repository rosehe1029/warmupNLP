{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks（RNNs）递归神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 的英文全称是 Recurrent Neural Networks ，即循环神经网络，是一种对序列型数据进行建模的深度模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t$:记忆体内当前时刻存储的状态信息\n",
    "\n",
    "$x_t$:当前时刻输入特征\n",
    "\n",
    "$h_{t-1}$:记忆体上一时刻存储的状态信息\n",
    "\n",
    "$y_t$:当前时刻循环核的输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_t=softmax(h_t  w_{hy} + b_y)$\n",
    "\n",
    "$h_t=tanh(x_t w_xh +h_{t-1} w_{hh}+b_h)$\n",
    "\n",
    "RNNs训练和传统ANN训练异同点？\n",
    "\n",
    "相同点：\n",
    "\n",
    "    RNNs与传统ANN都使用BP误差反向传播算法。\n",
    "\n",
    "不同点：\n",
    "\n",
    "    RNNs网络参数W,U,V是共享的，而传统神经网络各层参数间没有直接联系。\n",
    "    对于RNNs，在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，还依赖于之前若干步的网络状态。\n",
    "\n",
    "为什么RNN 训练的时候Loss波动很大\n",
    "\n",
    "​ 由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏。为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN中为什么会出现梯度消失,如何解决？\n",
    "\n",
    "RNN出现梯度消失和梯度爆炸主要体现在长句子中，因为在后向传播BP求导时，当前t时刻隐层输出的梯度包含了所有后续时刻激活函数导数的乘积，所以如果t越小、句子越长，就会出现问题。如果激活函数的导数特别小，累乘就会更小，则会出现梯度消失问题；反之，则是梯度爆炸问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。\n",
    "\n",
    "RNNs训练和传统ANN训练异同点？\n",
    "\n",
    "相同点：都使用BP误差反向传播算法。\n",
    "不同点：\n",
    "RNNs网络参数W,U,V是共享的，而传统神经网络各层参数间没有直接联系。\n",
    "对于RNNs，在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，还依赖于之前若干步的网络状态。\n",
    "\n",
    "为什么RNN 训练的时候Loss波动很大?\n",
    "\n",
    "由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，lr没法个性化的调整，导致RNN在train的过程中，Loss会震荡\n",
    "起伏，为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防\n",
    "止大幅震荡。\n",
    "\n",
    "RNN中为什么会出现梯度消失？\n",
    "\n",
    "\n",
    "梯度消失现象：累乘会导致激活函数导数的累乘，如果取tanh或sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是\n",
    "越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。\n",
    "实际使用中，会优先选择tanh函数，原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。\n",
    "\n",
    "$ \\prod_{j=k-1}^3{\\frac{\\partial s_j}{\\partial s_{j-1}}}=\\prod_{j=k-1}^3{tanh' W} $\n",
    "\n",
    "如何解决RNN中的梯度消失问题？\n",
    "\n",
    "1.选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数\n",
    "容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。\n",
    "2.加入BN层，其优点：加速收敛.控制过拟合，可以少用或不用Dropout和正则。降低网络对初始化权重不敏感，且能允许使用较大的学习\n",
    "率等。\n",
    "3.改变传播结构，LSTM结构可以有效解决这个问题。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn RNN(100, 20, num_layers=4)\n",
      "out.shape torch.Size([10, 1, 20])\n",
      "h.shape torch.Size([4, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "举例:设计一个4层的RNN,输入是一段中文，输出是一段英文，\n",
    "假设每个中文字符用100维数据进行编码，每个隐含层的维度是20,有4个隐含层\n",
    "input_size=100\n",
    "hidden_size=20\n",
    "num_layers=4\n",
    "再假设模型已经训练好了，现在有个长度为10的句子做输入，那么\n",
    "seq_len=10,batch_size=1\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size=100\n",
    "hidden_size=20\n",
    "num_layers=4\n",
    "\n",
    "rnn=nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)#,number_layers=number_layers)\n",
    "print(\"rnn\",rnn)\n",
    "\n",
    "seq_len=10\n",
    "batch_size=1\n",
    "x=torch.randn(seq_len,batch_size,input_size)\n",
    "h0=torch.zeros(num_layers,batch_size,hidden_size)\n",
    "\n",
    "out,h=rnn(x,h0)\n",
    "print(\"out.shape\",out.shape)\n",
    "print(\"h.shape\",h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      " Net(\n",
      "  (rnn): RNN(3, 16, batch_first=True)\n",
      "  (linear): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Iteration: 0 loss 13.739197731018066\n",
      "Iteration: 100 loss 0.8503149151802063\n",
      "Iteration: 200 loss 1.0509717464447021\n",
      "Iteration: 300 loss 0.8992526531219482\n",
      "Iteration: 400 loss 0.08515650779008865\n",
      "Iteration: 500 loss 0.006117912940680981\n",
      "Iteration: 600 loss 0.0017799311317503452\n",
      "Iteration: 700 loss 0.002400376135483384\n",
      "Iteration: 800 loss 0.0011865826090797782\n",
      "Iteration: 900 loss 0.0005421483656391501\n",
      "Iteration: 1000 loss 0.003756233723834157\n",
      "Iteration: 1100 loss 0.0009136120206676424\n",
      "Iteration: 1200 loss 0.00035660137655213475\n",
      "Iteration: 1300 loss 0.00041771383257582784\n",
      "Iteration: 1400 loss 0.0009116978035308421\n",
      "Iteration: 1500 loss 0.000408749416237697\n",
      "Iteration: 1600 loss 0.0003605236706789583\n",
      "Iteration: 1700 loss 0.00035006814869120717\n",
      "Iteration: 1800 loss 0.00048723191139288247\n",
      "Iteration: 1900 loss 0.00035314919659867883\n",
      "Iteration: 2000 loss 0.000793592946138233\n",
      "Iteration: 2100 loss 0.0003381486749276519\n",
      "Iteration: 2200 loss 0.0004723184974864125\n",
      "Iteration: 2300 loss 0.00048184936167672276\n",
      "Iteration: 2400 loss 0.0006906306371092796\n",
      "Iteration: 2500 loss 0.0003826722386293113\n",
      "Iteration: 2600 loss 0.0002897434460464865\n",
      "Iteration: 2700 loss 0.0003416710824239999\n",
      "Iteration: 2800 loss 0.0006339274113997817\n",
      "Iteration: 2900 loss 0.0005273947026580572\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "RNN做时序数据预测\n",
    "假设现在有一系列3维飞机航迹数据，我们想预测接下来的航迹数据，那么可以考虑用RNN预测\n",
    "。首先设计网络，每个航迹点都是3维的，所以input_size = 3，隐含层\n",
    "hidden_size = 16，有一个隐含层，所以num_layers = 1。为了更好的利用数据，\n",
    "下面代码实现的是这样的功能：输入第[1,15]个数据，输出第[6,21]个数据，\n",
    "即往后平移5个单位的数据。\n",
    "'''\n",
    "import  torch\n",
    "import datetime\n",
    "import  numpy as np\n",
    "import  torch.nn as nn\n",
    "import  torch.optim as optim\n",
    "from    matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['FangSong']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "###########################设置全局变量###################################\n",
    "\n",
    "num_time_steps = 16    # 训练时时间窗的步长\n",
    "input_size = 3          # 输入数据维度\n",
    "hidden_size = 16        # 隐含层维度\n",
    "output_size = 3         # 输出维度\n",
    "num_layers = 1\n",
    "lr=0.01\n",
    "####################定义RNN类##############################################\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        for p in self.rnn.parameters():\n",
    "          nn.init.normal_(p, mean=0.0, std=0.001)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_prev):\n",
    "\n",
    "       out, hidden_prev = self.rnn(x, hidden_prev)\n",
    "       # [b, seq, h]\n",
    "       out = out.view(-1, hidden_size)\n",
    "       out = self.linear(out)#[seq,h] => [seq,3]\n",
    "       out = out.unsqueeze(dim=0)  # => [1,seq,3]\n",
    "       return out, hidden_prev\n",
    "\n",
    "####################初始化训练集#################################\n",
    "def getdata():\n",
    "    x1 = np.linspace(1,10,30).reshape(30,1)\n",
    "    y1 = (np.zeros_like(x1)+2)+np.random.rand(30,1)*0.1\n",
    "    z1 = (np.zeros_like(x1)+2).reshape(30,1)\n",
    "    tr1 =  np.concatenate((x1,y1,z1),axis=1)\n",
    "    # mm = MinMaxScaler()\n",
    "    # data = mm.fit_transform(tr1)   #数据归一化\n",
    "    return tr1\n",
    "\n",
    "#####################开始训练模型#################################\n",
    "def tarin_RNN(data):\n",
    "\n",
    "    model = Net(input_size, hidden_size, num_layers)\n",
    "    print('model:\\n',model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    #初始化h\n",
    "    hidden_prev = torch.zeros(1, 1, hidden_size)\n",
    "    l = []\n",
    "    # 训练3000次\n",
    "    for iter in range(3000):\n",
    "        # loss = 0\n",
    "        start = np.random.randint(10, size=1)[0]\n",
    "        end = start + 15\n",
    "        x = torch.tensor(data[start:end]).float().view(1, num_time_steps - 1, 3)\n",
    "        # 在data里面随机选择15个点作为输入，预测第16\n",
    "        y = torch.tensor(data[start + 5:end + 5]).float().view(1, num_time_steps - 1, 3)\n",
    "\n",
    "        output, hidden_prev = model(x, hidden_prev)\n",
    "        hidden_prev = hidden_prev.detach()\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Iteration: {} loss {}\".format(iter, loss.item()))\n",
    "            l.append(loss.item())\n",
    "\n",
    "\n",
    "    ##############################绘制损失函数#################################\n",
    "    plt.plot(l,'r')\n",
    "    plt.xlabel('训练次数')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('RNN损失函数下降曲线')\n",
    "\n",
    "    return hidden_prev,model\n",
    "#############################预测#########################################\n",
    "\n",
    "def RNN_pre(model,data,hidden_prev):\n",
    "    data_test = data[19:29]\n",
    "    data_test = torch.tensor(np.expand_dims(data_test, axis=0),dtype=torch.float32)\n",
    "\n",
    "    pred1,h1 = model(data_test,hidden_prev )\n",
    "    print('pred1.shape:',pred1.shape)\n",
    "    pred2,h2 = model(pred1,hidden_prev )\n",
    "    print('pred2.shape:',pred2.shape)\n",
    "    pred1 = pred1.detach().numpy().reshape(10,3)\n",
    "    pred2 = pred2.detach().numpy().reshape(10,3)\n",
    "    predictions = np.concatenate((pred1,pred2),axis=0)\n",
    "    # predictions= mm.inverse_transform(predictions)\n",
    "    print('predictions.shape:',predictions.shape)\n",
    "\n",
    "    #############################预测可视化########################################\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    ax = Axes3D(fig)\n",
    "    ax.scatter3D(data[:, 0],data[:, 1],data[:,2],c='red')\n",
    "    ax.scatter3D(predictions[:,0],predictions[:,1],predictions[:,2],c='y')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_xlim(0, 8.5)\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_zlim(0, 4)\n",
    "    plt.title(\"RNN航迹预测\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    data = getdata()\n",
    "    start = datetime.datetime.now()\n",
    "    hidden_pre, model = tarin_RNN(data)\n",
    "    end = datetime.datetime.now()\n",
    "    print('The training time: %s' % str(end - start))\n",
    "    plt.show()\n",
    "    RNN_pre(model, data, hidden_pre)\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\RNNLSTMGRU\\RNN相关.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 126>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000012?line=121'>122</a>\u001b[0m test_X, test_y \u001b[39m=\u001b[39m generate_data(np\u001b[39m.\u001b[39msin(np\u001b[39m.\u001b[39mlinspace(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000012?line=122'>123</a>\u001b[0m     test_start, test_end, TESTING_EXAMPLES \u001b[39m+\u001b[39m TIMESTEPS, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000012?line=124'>125</a>\u001b[0m \u001b[39m#maxi:可以看到最后的步骤就是先训练模型，然后把训练出的模型拿去预测\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000012?line=125'>126</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39;49mSession() \u001b[39mas\u001b[39;00m sess:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000012?line=126'>127</a>\u001b[0m     train(sess, train_X, train_y)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000012?line=127'>128</a>\u001b[0m     run_eval(sess, test_X, test_y)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# 1. 定义RNN的参数\n",
    "HIDDEN_SIZE = 30                            # LSTM中隐藏节点的个数。\n",
    "NUM_LAYERS = 2                              # LSTM的层数。\n",
    "TIMESTEPS = 10                              # 循环神经网络的训练序列长度。\n",
    "TRAINING_STEPS = 10000                      # 训练轮数。\n",
    "BATCH_SIZE = 32                             # batch大小。\n",
    "TRAINING_EXAMPLES = 10000                   # 训练数据个数。\n",
    "TESTING_EXAMPLES = 1000                     # 测试数据个数。\n",
    "SAMPLE_GAP = 0.01                           # 采样间隔。\n",
    " \n",
    " \n",
    "# 2. 产生正弦数据函数\n",
    "def generate_data(seq):\n",
    "    X = []\n",
    "    y = []\n",
    "    # 序列的第i项和后面的TIMESTEPS-1项合在一起作为输入；第i + TIMESTEPS项作为输出。\n",
    "    # 即用sin函数前面的TIMESTEPS个点的信息，预测第i + TIMESTEPS个点的函数值。\n",
    "    for i in range(len(seq) - TIMESTEPS):\n",
    "        X.append([seq[i: i + TIMESTEPS]])\n",
    "        y.append([seq[i + TIMESTEPS]])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)  \n",
    " \n",
    " \n",
    "# 3. 定义网络结构和优化步骤\n",
    "def lstm_model(X, y, is_training):\n",
    "    # 使用多层的LSTM结构。\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([\n",
    "        tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)\n",
    "        for _ in range(NUM_LAYERS)]) \n",
    " \n",
    "    # 使用TensorFlow接口将多层的LSTM结构连接成RNN网络并计算其前向传播结果。\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    # outputs是顶层LSTM在每一步的输出结果，它的维度是[batch_size, time ,\n",
    "    # HIDDEN_SIZE]。在本问题中只关注最后一个时刻的输出结果。\n",
    "    output = outputs[:, -1, :]\n",
    " \n",
    "    # 对LSTM网络的输出再做加一层全链接层并计算损失。注意这里默认的损失为平均\n",
    "    # 平方差损失函数。\n",
    "    predictions = tf.contrib.layers.fully_connected(\n",
    "        output, 1, activation_fn=None)\n",
    "    \n",
    "    # 只在训练时计算损失函数和优化步骤。测试时直接返回预测结果。\n",
    "    if not is_training:\n",
    "        return predictions, None, None\n",
    "        \n",
    "    # 计算损失函数。\n",
    "    loss = tf.losses.mean_squared_error(labels=y, predictions=predictions)\n",
    " \n",
    "    # 创建模型优化器并得到优化步骤。\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss, tf.train.get_global_step(),\n",
    "        optimizer=\"Adagrad\", learning_rate=0.1)\n",
    "    \n",
    "    return predictions, loss, train_op\n",
    " \n",
    " \n",
    "# 4. 定义训练方法\n",
    "def train(sess, train_X, train_Y):\n",
    "    # 将训练数据以数据集的方式提供给计算图\n",
    "    ds = tf.data.Dataset.from_tensor_slices((train_X, train_Y))\n",
    "    ds = ds.repeat().shuffle(1000).batch(BATCH_SIZE)   #maxi:这种该怎么理解\n",
    "    X, y = ds.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    # 定义模型，得到预测结果、损失函数，和训练操作。\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        _, loss, train_op = lstm_model(X, y, True)\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        _, l = sess.run([train_op, loss])\n",
    "        if i % 1000 == 0:\n",
    "            print(\"train step: \" + str(i) + \", loss: \", str(l))\n",
    "            \n",
    " \n",
    "# 5. 定义测试方法\n",
    "def run_eval(sess, test_X, test_y):\n",
    "    # 将测试数据以数据集的方式提供给计算图。\n",
    "    ds = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
    "    ds = ds.batch(1)\n",
    "    X, y = ds.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    # 调用模型得到计算结果。这里不需要输入真实的y值。\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        prediction, _, _ = lstm_model(X, [0.0], False)\n",
    "    \n",
    "    # 将预测结果存入一个数组。\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in range(TESTING_EXAMPLES):\n",
    "        p, l = sess.run([prediction, y])\n",
    "        predictions.append(p)\n",
    "        labels.append(l)\n",
    " \n",
    "    # 计算rmse作为评价指标。\n",
    "    predictions = np.array(predictions).squeeze()\n",
    "    labels = np.array(labels).squeeze()\n",
    "    rmse = np.sqrt(((predictions - labels) ** 2).mean(axis=0))\n",
    "    print(\"Root Mean Square Error is: %f\" % rmse)\n",
    "    \n",
    "    # 对预测的sin函数曲线进行绘图。\n",
    "    plt.figure()\n",
    "    plt.plot(predictions, label='predictions')\n",
    "    plt.plot(labels, label='real_sin')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# 6. 生成数据并训练、验证\n",
    "# 用正弦函数生成训练和测试数据集合。\n",
    "# numpy.linspace函数可以创建一个等差序列的数组，它常用的参数有三个参数，\n",
    "# 第一个参数表示起始值，第二个参数表示终止值，第三个参数表示数列的长度。\n",
    "# 例如linespace(1, 10, 10)产生的数组是arrray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) \n",
    "test_start = (TRAINING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP\n",
    "test_end = test_start + (TESTING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP\n",
    "train_X, train_y = generate_data(np.sin(np.linspace(\n",
    "    0, test_start, TRAINING_EXAMPLES + TIMESTEPS, dtype=np.float32)))\n",
    "test_X, test_y = generate_data(np.sin(np.linspace(\n",
    "    test_start, test_end, TESTING_EXAMPLES + TIMESTEPS, dtype=np.float32)))\n",
    " \n",
    "#maxi:可以看到最后的步骤就是先训练模型，然后把训练出的模型拿去预测\n",
    "with tf.Session() as sess:\n",
    "    train(sess, train_X, train_y)\n",
    "    run_eval(sess, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "#print(findFiles('data/names/*.txt'))\n",
    "print(findFiles('E:\\坚果云\\精益求精\\warmup\\RNNLSTMGRU\\data\\datanames\\*.txt'))\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Italian'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\RNNLSTMGRU\\RNN相关.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/RNNLSTMGRU/RNN%E7%9B%B8%E5%85%B3.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(category_lines[\u001b[39m'\u001b[39;49m\u001b[39mItalian\u001b[39;49m\u001b[39m'\u001b[39;49m][:\u001b[39m5\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Italian'"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
