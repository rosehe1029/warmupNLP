{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02353202486135686, 0.03825262795920514, 0.001301216947797168, 0.24856292688104265, 0.15617721411082464]\n",
      "[6.115357109592878e-10, 0.09666549769993338, 3.183231456205249e-12, 8.835204937662766e-07, 0.016844086122281388]\n",
      "[0.07254786994253012, 0.08600248954152065, 0.05549865026325765, 0.07969999429142749, 0.22759543097204027]\n",
      "[0.1631348895370225, 0.08097127589483341, 0.10276367098051566, 0.16719709509315875, 0.12023090695813965]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "def test_gradient(dim, time_steps=50, scale=1.0):\n",
    "    # Assume components of the query and keys are drawn from N(0, 1) independently\n",
    "    q = np.random.randn(dim)\n",
    "    ks = np.random.randn(time_steps, dim)\n",
    "    x = np.sum(q * ks, axis=1) / scale  # x.shape = (time_steps,) \n",
    "    y = softmax(x)\n",
    "    grad = np.diag(y) - np.outer(y, y)\n",
    "    return np.max(np.abs(grad))  # the maximum component of gradients\n",
    "\n",
    "NUMBER_OF_EXPERIMENTS = 5\n",
    "# results of 5 random runs without scaling\n",
    "print([test_gradient(100) for _ in range(NUMBER_OF_EXPERIMENTS)])\n",
    "print([test_gradient(1000) for _ in range(NUMBER_OF_EXPERIMENTS)])\n",
    "\n",
    "# results of 5 random runs with scaling\n",
    "print([test_gradient(100, scale=np.sqrt(100)) for _ in range(NUMBER_OF_EXPERIMENTS)])\n",
    "print([test_gradient(1000, scale=np.sqrt(1000)) for _ in range(NUMBER_OF_EXPERIMENTS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\tdef __init__(self, embed_size, heads):\n",
    "\t\tsuper(SelfAttention, self).__init__()\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.heads = heads\n",
    "\t\tself.head_dim = embed_size // heads\n",
    "\n",
    "\t\tassert (self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "\n",
    "\t\tself.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\t\tself.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\t\tself.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\t\tself.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "\tdef forward(self, values, keys, query, mask):\n",
    "\t\tN =query.shape[0]\n",
    "\t\tvalue_len , key_len , query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "\t\t# split embedding into self.heads pieces\n",
    "\t\tvalues = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "\t\tkeys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "\t\tqueries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\t\t\n",
    "\t\tvalues = self.values(values)\n",
    "\t\tkeys = self.keys(keys)\n",
    "\t\tqueries = self.queries(queries)\n",
    "\n",
    "\t\tenergy = torch.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
    "\t\t# queries shape: (N, query_len, heads, heads_dim)\n",
    "\t\t# keys shape : (N, key_len, heads, heads_dim)\n",
    "\t\t# energy shape: (N, heads, query_len, key_len)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\tenergy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "\t\tattention = torch.softmax(energy/ (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "\t\tout = torch.einsum(\"nhql, nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n",
    "\t\t# attention shape: (N, heads, query_len, key_len)\n",
    "\t\t# values shape: (N, value_len, heads, heads_dim)\n",
    "\t\t# (N, query_len, heads, head_dim)\n",
    "\n",
    "\t\tout = self.fc_out(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "\t\tsuper(TransformerBlock, self).__init__()\n",
    "\t\tself.attention = SelfAttention(embed_size, heads)\n",
    "\t\tself.norm1 = nn.LayerNorm(embed_size)\n",
    "\t\tself.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "\t\tself.feed_forward = nn.Sequential(\n",
    "\t\t\tnn.Linear(embed_size, forward_expansion*embed_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(forward_expansion*embed_size, embed_size)\n",
    "\t\t)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, value, key, query, mask):\n",
    "\t\tattention = self.attention(value, key, query, mask)\n",
    "\n",
    "\t\tx = self.dropout(self.norm1(attention + query))\n",
    "\t\tforward = self.feed_forward(x)\n",
    "\t\tout = self.dropout(self.norm2(forward + x))\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tsrc_vocab_size,\n",
    "\t\t\tembed_size,\n",
    "\t\t\tnum_layers,\n",
    "\t\t\theads,\n",
    "\t\t\tdevice,\n",
    "\t\t\tforward_expansion,\n",
    "\t\t\tdropout,\n",
    "\t\t\tmax_length,\n",
    "\t\t):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.device = device\n",
    "\t\tself.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "\t\tself.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[\n",
    "\t\t\t\tTransformerBlock(\n",
    "\t\t\t\t\tembed_size,\n",
    "\t\t\t\t\theads,\n",
    "\t\t\t\t\tdropout=dropout,\n",
    "\t\t\t\t\tforward_expansion=forward_expansion,\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\tfor _ in range(num_layers)]\n",
    "\t\t)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tN, seq_length = x.shape\n",
    "\t\tpositions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\t\tout = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tout = layer(out, out, out, mask)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\tdef __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "\t\tsuper(DecoderBlock, self).__init__()\n",
    "\t\tself.attention = SelfAttention(embed_size, heads)\n",
    "\t\tself.norm = nn.LayerNorm(embed_size)\n",
    "\t\tself.transformer_block = TransformerBlock(\n",
    "\t\t\tembed_size, heads, dropout, forward_expansion\n",
    "\t\t)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x, value, key, src_mask, trg_mask):\n",
    "\t\tattention = self.attention(x, x, x, trg_mask)\n",
    "\t\tquery = self.dropout(self.norm(attention + x))\n",
    "\t\tout = self.transformer_block(value, key, query, src_mask)\n",
    "\t\treturn out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\ttrg_vocab_size,\n",
    "\t\t\tembed_size,\n",
    "\t\t\tnum_layers,\n",
    "\t\t\theads,\n",
    "\t\t\tforward_expansion,\n",
    "\t\t\tdropout,\n",
    "\t\t\tdevice,\n",
    "\t\t\tmax_length,\n",
    "\t):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.device = device\n",
    "\t\tself.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "\t\tself.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "\t\t\tfor _ in range(num_layers)]\n",
    "\t\t\t)\n",
    "\t\tself.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x ,enc_out , src_mask, trg_mask):\n",
    "\t\tN, seq_length = x.shape\n",
    "\t\tpositions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\t\tx = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "\t\tout =self.fc_out(x)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tsrc_vocab_size,\n",
    "\t\t\ttrg_vocab_size,\n",
    "\t\t\tsrc_pad_idx,\n",
    "\t\t\ttrg_pad_idx,\n",
    "\t\t\tembed_size = 256,\n",
    "\t\t\tnum_layers = 6,\n",
    "\t\t\tforward_expansion = 4,\n",
    "\t\t\theads = 8,\n",
    "\t\t\tdropout = 0,\n",
    "\t\t\tdevice=\"cuda\",\n",
    "\t\t\tmax_length=100\n",
    "\t\t):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.encoder = Encoder(\n",
    "\t\t\tsrc_vocab_size,\n",
    "\t\t\tembed_size,\n",
    "\t\t\tnum_layers,\n",
    "\t\t\theads,\n",
    "\t\t\tdevice,\n",
    "\t\t\tforward_expansion,\n",
    "\t\t\tdropout,\n",
    "\t\t\tmax_length\n",
    "\t\t\t)\n",
    "\t\tself.decoder = Decoder(\n",
    "\t\t\ttrg_vocab_size,\n",
    "\t\t\tembed_size,\n",
    "\t\t\tnum_layers,\n",
    "\t\t\theads,\n",
    "\t\t\tforward_expansion,\n",
    "\t\t\tdropout,\n",
    "\t\t\tdevice,\n",
    "\t\t\tmax_length\n",
    "\t\t\t)\n",
    "\n",
    "\n",
    "\t\tself.src_pad_idx = src_pad_idx\n",
    "\t\tself.trg_pad_idx = trg_pad_idx\n",
    "\t\tself.device = device\n",
    "\n",
    "\n",
    "\tdef make_src_mask(self, src):\n",
    "\t\tsrc_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\t\t# (N, 1, 1, src_len)\n",
    "\t\treturn src_mask.to(self.device)\n",
    "\n",
    "\tdef make_trg_mask(self, trg):\n",
    "\t\tN, trg_len = trg.shape\n",
    "\t\ttrg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "\t\t\tN, 1, trg_len, trg_len\n",
    "\t\t)\n",
    "\t\treturn trg_mask.to(self.device)\n",
    "\n",
    "\tdef forward(self, src, trg):\n",
    "\t\tsrc_mask = self.make_src_mask(src)\n",
    "\t\ttrg_mask = self.make_trg_mask(trg)\n",
    "\t\tenc_src = self.encoder(src, src_mask)\n",
    "\t\tout = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(device)\n",
    "\tx = torch.tensor([[1,5,6,4,3,9,5,2,0],[1,8,7,3,4,5,6,7,2]]).to(device)\n",
    "\ttrg = torch.tensor([[1,7,4,3,5,9,2,0],[1,5,6,2,4,7,6,2]]).to(device)\n",
    "\n",
    "\tsrc_pad_idx = 0\n",
    "\ttrg_pad_idx = 0\n",
    "\tsrc_vocab_size = 10\n",
    "\ttrg_vocab_size = 10\n",
    "\tmodel = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
    "\tout = model(x, trg[:, : -1])\n",
    "\tprint(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import embedding\n",
    "\n",
    "\n",
    "embedding=nn.Embedding(10,3)\n",
    "#词表大小为10，词嵌入的维度为3\n",
    "input1=torch.LongTensor([[1,2,4,5],[4,3,3,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ember: tensor([[[-12.7855, -18.9745,  38.5586,  ...,  -0.3868, -42.8885, -30.1096],\n",
      "         [ 13.6334, -21.2008, -16.9512,  ..., -17.3758, -15.3683,  13.4025],\n",
      "         [  7.3512,  20.1956, -22.5922,  ...,  21.5400, -21.9771, -41.2900],\n",
      "         [  2.6230,  17.0199,   2.0810,  ...,   5.1560, -10.6985,   7.5601]],\n",
      "\n",
      "        [[ -8.1162,  56.3541,   0.6642,  ...,  33.7957,   1.4876,  -3.2103],\n",
      "         [ 23.3764, -12.2481, -40.7027,  ...,  -6.1936, -49.8983,  13.4108],\n",
      "         [  9.4422, -14.1022,   4.6149,  ...,  -0.3878,  -4.8056, -17.9027],\n",
      "         [ -5.4352,   2.0048,   3.8061,  ...,   8.2661,   7.3232, -32.0618]]],\n",
      "       grad_fn=<MulBackward0>) \n",
      "*********\n",
      " torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# 构建 Embedding 类来实现文本嵌入层\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model):\n",
    "        # vocab： 词表的大小\n",
    "        # d_model : 词嵌入的维度\n",
    "        super(Embedding, self).__init__()\n",
    "        # 定义 Embedding 层\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # 将参数传入类中国\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: 代表的是输入进模型的文本，通过词汇映射后的数字张量\n",
    "        return self.lut(x) * math.sqrt(self.d_model) \n",
    "\n",
    "\n",
    "# 词表： 1000*512， 共是1000个词，每一行是一个词，每个词是一个512d的向量表示\n",
    "vocab = 1000\n",
    "d_model = 512\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "print(\"ember:\", embr, \"\\n*********\\n\", embr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'embedding_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\Transformer\\transformer.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000007?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000007?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39membedding_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000007?line=6'>7</a>\u001b[0m \u001b[39m# 构建位置编码器的类\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000007?line=7'>8</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPositionalEncoding\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'embedding_layer'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "from embedding_layer import Embedding\n",
    "\n",
    "# 构建位置编码器的类\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        # d_model : 代表词嵌入的维度\n",
    "        # dropout : 代表Dropout层的置零比率\n",
    "        # max_len : 代表每个句子的最大长度\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # 实例化 Dropout层\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化一个位置编码矩阵，大小是 max_len * d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 初始化一个绝对位置矩阵, max_len * 1\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position)\n",
    "\n",
    "        # 定义一个变化矩阵，div_term, 跳跃式的初始化\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        print(\"\\ndiv_term\", div_term)\n",
    "\n",
    "        # 将前面定义的变化矩阵 进行技术，偶数分别赋值\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 用正弦波给偶数部分赋值\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 用余弦波给奇数部分赋值\n",
    "\n",
    "        # 将二维张量，扩充为三维张量\n",
    "        pe = pe.unsqueeze(0)  # 1 * max_len * d_model\n",
    "\n",
    "        # 将位置编码矩阵，注册成模型的buffer，这个buffer不是模型中的参数，不跟随优化器同步更新\n",
    "        # 注册成buffer后，就可以在模型保存后 重新加载的时候，将这个位置编码器和模型参数\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 代表文本序列的词嵌入表示\n",
    "        # 首先明确pe的编码太长了，将第二个维度，就是max_len对应的维度，缩小成x的句子的同等的长度\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)  # 表示位置编码是不参与更新的\n",
    "        return self.dropout(x)\n",
    "\n",
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 60\n",
    "vocab = 1000\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "x = embr  # shape: [2, 4, 512]\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "print(pe_result)\n",
    "print(\"\\n*********\\n\", pe_result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'position'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\Transformer\\transformer.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PositionalEncoding\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Variable\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'position'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from position import PositionalEncoding\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "pe = PositionalEncoding(20, 0)  # 实例化这个模型，d_model = 20, dropout = 0\n",
    "y = pe(Variable(torch.zeros(1, 100, 20)))  # 相当于只看位置矩阵\n",
    "\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "\n",
    "# 在画布上填写维度提示信息\n",
    "plt.legend([\"dim %d\" %p for p in [4, 5, 6, 7]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    # size: 代表掩码张量 最后两个维度，形成一个方阵\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # 使用np.ones()先构建一个全1 的张量，然后用np.triu形成上三角矩阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # 反转\n",
    "    return torch.from_numpy(1-subsequent_mask)\n",
    "\n",
    "size = 5\n",
    "sm = subsequent_mask(size)\n",
    "print(sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pe_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\Transformer\\transformer.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000011?line=30'>31</a>\u001b[0m     \u001b[39m# 最后一步完成p_attm 和 value 的乘法，并返回query的注意力表示\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000011?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmatmul(p_attn, value), p_attn\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000011?line=34'>35</a>\u001b[0m query \u001b[39m=\u001b[39m key \u001b[39m=\u001b[39m value \u001b[39m=\u001b[39m pe_result\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000011?line=35'>36</a>\u001b[0m mask \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000011?line=36'>37</a>\u001b[0m attn, p_attn \u001b[39m=\u001b[39m attention(query, key, value, mask\u001b[39m=\u001b[39mmask)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pe_result' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # query, key, value : 代表注意力的三个输入张量\n",
    "    # mask : 掩码张量\n",
    "    # dropout : 传入Dropout实例化对象\n",
    "    # 首先，将query的最后一个维度提取出来，代表的是词嵌入的维度\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 按照注意力计算公式，将query和key 的转置进行矩阵乘法，然后除以缩放系数\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # 判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 利用masked_fill 方法，将掩码张量和0进行位置的意义比较，如果等于0，就替换成 -1e9\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # scores的最后一个维度上进行 softmax\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 判断是否使用dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后一步完成p_attm 和 value 的乘法，并返回query的注意力表示\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "print('attn', attn)\n",
    "print('attn.shape', attn.shape)\n",
    "print(\"p_attn\", p_attn)\n",
    "print(p_attn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clones(module, N):\n",
    "    # module : 代表要克隆的目标网络层\n",
    "    # N : 将module几个\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'embedding_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\坚果云\\精益求精\\warmup\\warmupNLP\\Transformer\\transformer.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000013?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39membedding_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E5%9D%9A%E6%9E%9C%E4%BA%91/%E7%B2%BE%E7%9B%8A%E6%B1%82%E7%B2%BE/warmup/warmupNLP/Transformer/transformer.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39m# 构建位置编码器的类\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'embedding_layer'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "from embedding_layer import Embedding\n",
    "import copy\n",
    "\n",
    "# 构建位置编码器的类\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        # d_model : 代表词嵌入的维度\n",
    "        # dropout : 代表Dropout层的置零比率\n",
    "        # max_len : 代表每个句子的最大长度\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # 实例化 Dropout层\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化一个位置编码矩阵，大小是 max_len * d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 初始化一个绝对位置矩阵, max_len * 1\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # print(position)\n",
    "\n",
    "        # 定义一个变化矩阵，div_term, 跳跃式的初始化\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        # print(\"ndiv_term\", div_term)\n",
    "\n",
    "        # 将前面定义的变化矩阵 进行技术，偶数分别赋值\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 用正弦波给偶数部分赋值\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 用余弦波给奇数部分赋值\n",
    "\n",
    "        # 将二维张量，扩充为三维张量\n",
    "        pe = pe.unsqueeze(0)  # 1 * max_len * d_model\n",
    "\n",
    "        # 将位置编码矩阵，注册成模型的buffer，这个buffer不是模型中的参数，不跟随优化器同步更新\n",
    "        # 注册成buffer后，就可以在模型保存后 重新加载的时候，将这个位置编码器和模型参数\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 代表文本序列的词嵌入表示\n",
    "        # 首先明确pe的编码太长了，将第二个维度，就是max_len对应的维度，缩小成x的句子的同等的长度\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)  # 表示位置编码是不参与更新的\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 60\n",
    "vocab = 1000\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "x = embr  # shape: [2, 4, 512]\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "# print(pe_result)\n",
    "\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # query, key, value : 代表注意力的三个输入张量\n",
    "    # mask : 掩码张量\n",
    "    # dropout : 传入Dropout实例化对象\n",
    "    # 首先，将query的最后一个维度提取出来，代表的是词嵌入的维度\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 按照注意力计算公式，将query和key 的转置进行矩阵乘法，然后除以缩放系数\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    print(\"..\", scores.shape)\n",
    "    # 判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 利用masked_fill 方法，将掩码张量和0进行位置的意义比较，如果等于0，就替换成 -1e9\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # scores的最后一个维度上进行 softmax\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 判断是否使用dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后一步完成p_attm 和 value 的乘法，并返回query的注意力表示\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "# print('attn', attn)\n",
    "# print('attn.shape', attn.shape)\n",
    "# print(\"p_attn\", p_attn)\n",
    "# print(p_attn.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 实现克隆函数，因为在多头注意力机制下，要用到多个结果相同的线性层\n",
    "# 需要使用clone 函数u，将他们统一 初始化到一个网络层列表对象中\n",
    "def clones(module, N):\n",
    "    # module : 代表要克隆的目标网络层\n",
    "    # N : 将module几个\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "# 实现多头注意力机制的类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, dropout=0.1):\n",
    "        # head : 代表几个头的函数\n",
    "        # embedding_dim ： 代表词嵌入的维度\n",
    "        # dropout\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # 强调：多头的数量head 需要整除 词嵌入的维度 embedding_dim\n",
    "        assert embedding_dim % head == 0\n",
    "\n",
    "        # 得到每个头，所获得 的词向量的维度\n",
    "        self.d_k = embedding_dim // head\n",
    "        self.head = head\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 获得线性层，需要获得4个，分别是Q K V 以及最终输出的线性层\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "\n",
    "        # 初始化注意力张量\n",
    "        self.attn = None\n",
    "\n",
    "        # 初始化dropout对象\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query,key,value 是注意力机制的三个输入张量，mask代表掩码张量\n",
    "        # 首先判断是否使用掩码张量\n",
    "        if mask is not None:\n",
    "            # 使用squeeze将掩码张量进行围堵扩充，代表多头的第n个头\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 得到batch_size\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 首先使用 zip 将网络能和输入数据连接在一起，模型的输出 利用 view 和 transpose 进行维度和形状的\n",
    "        query, key, value = \\\n",
    "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)\n",
    "             for model, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 将每个头的输出 传入到注意力层\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.drop)\n",
    "\n",
    "        # 得到每个头的计算结果，每个output都是4维的张量，需要进行维度转换\n",
    "        # 前面已经将transpose（1， 2）\n",
    "        # 注意，先transpose 然后 contiguous，否则无法使用view\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "\n",
    "        # 最后将x输入到线性层的最后一个线性层中进行处理，得到最终的多头注意力结构输出\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "# 实例化若干个参数\n",
    "head = 8\n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# 若干输入参数的初始化\n",
    "query = key = value = pe_result\n",
    "\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "mha = MultiHeadAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)\n",
    "\n",
    "print(mha_result)\n",
    "print(mha_result.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5861, -0.0117,  1.2004,  ..., -0.6232,  0.3535,  1.3491],\n",
      "         [ 0.8099, -0.5379,  0.8348,  ..., -1.5828, -0.6371,  0.9689],\n",
      "         [-1.1210, -0.2339,  1.6161,  ..., -1.4314,  0.8251,  1.0368],\n",
      "         [-0.1154, -0.0739,  1.6003,  ..., -0.8956,  0.2955,  1.7212]],\n",
      "\n",
      "        [[ 0.1560, -0.4482,  1.1628,  ..., -0.6699, -0.3188,  1.3650],\n",
      "         [ 0.2348, -0.1948,  1.3426,  ..., -1.5506,  0.2838,  1.4310],\n",
      "         [ 1.1146,  0.2120, -0.5381,  ..., -0.9989,  0.0524,  0.5629],\n",
      "         [ 0.6154, -0.2227,  0.8482,  ...,  0.3371, -0.7661,  0.6899]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# main作用：集成了整个Transformer代码\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "# 构建 Embedding 类来实现文本嵌入层\n",
    "# vocab : 词表的长度， d_model : 词嵌入的维度\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "# 词表： 1000*512， 共是1000个词，每一行是一个词，每个词是一个512d的向量表示\n",
    "vocab = 1000\n",
    "d_model = 512\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "\n",
    "########################################################################################################################\n",
    "# 构建位置编码器的类\n",
    "# d_model : 代表词嵌入的维度\n",
    "# dropout : 代表Dropout层的置零比率\n",
    "# max_len : 代表每个句子的最大长度\n",
    "# 初始化一个位置编码矩阵pe，大小是 max_len * d_model\n",
    "# 初始化一个绝对位置矩阵position, 大小是max_len * 1\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 定义一个变化矩阵，div_term, 跳跃式的初始化\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # 将前面定义的变化矩阵 进行技术，偶数分别赋值\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 用正弦波给偶数部分赋值\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 用余弦波给奇数部分赋值\n",
    "\n",
    "        # 将二维张量，扩充为三维张量\n",
    "        pe = pe.unsqueeze(0)  # 1 * max_len * d_model\n",
    "\n",
    "        # 将位置编码矩阵，注册成模型的buffer，这个buffer不是模型中的参数，不跟随优化器同步更新\n",
    "        # 注册成buffer后，就可以在模型保存后 重新加载的时候，将这个位置编码器和模型参数\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 代表文本序列的词嵌入表示\n",
    "        # 首先明确pe的编码太长了，将第二个维度，就是max_len对应的维度，缩小成x的句子的同等的长度\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)  # 表示位置编码是不参与更新的\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 60\n",
    "vocab = 1000\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "x = embr  # shape: [2, 4, 512]\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "# print(pe_result)\n",
    "\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # query, key, value : 代表注意力的三个输入张量\n",
    "    # mask : 掩码张量\n",
    "    # dropout : 传入Dropout实例化对象\n",
    "    # 首先，将query的最后一个维度提取出来，代表的是词嵌入的维度\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 按照注意力计算公式，将query和key 的转置进行矩阵乘法，然后除以缩放系数\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # print(\"..\", scores.shape)\n",
    "    # 判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 利用masked_fill 方法，将掩码张量和0进行位置的意义比较，如果等于0，就替换成 -1e9\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # scores的最后一个维度上进行 softmax\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 判断是否使用dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后一步完成p_attm 和 value 的乘法，并返回query的注意力表示\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "# print('attn', attn)\n",
    "# print('attn.shape', attn.shape)\n",
    "# print(\"p_attn\", p_attn)\n",
    "# print(p_attn.shape)\n",
    "\n",
    "\n",
    "# 实现克隆函数，因为在多头注意力机制下，要用到多个结果相同的线性层\n",
    "# 需要使用clone 函数u，将他们统一 初始化到一个网络层列表对象中\n",
    "def clones(module, N):\n",
    "    # module : 代表要克隆的目标网络层\n",
    "    # N : 将module几个\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "# 实现多头注意力机制的类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, dropout=0.1):\n",
    "        # head : 代表几个头的函数\n",
    "        # embedding_dim ： 代表词嵌入的维度\n",
    "        # dropout\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # 强调：多头的数量head 需要整除 词嵌入的维度 embedding_dim\n",
    "        assert embedding_dim % head == 0\n",
    "\n",
    "        # 得到每个头，所获得 的词向量的维度\n",
    "        self.d_k = embedding_dim // head\n",
    "        self.head = head\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 获得线性层，需要获得4个，分别是Q K V 以及最终输出的线性层\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "\n",
    "        # 初始化注意力张量\n",
    "        self.attn = None\n",
    "\n",
    "        # 初始化dropout对象\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query,key,value 是注意力机制的三个输入张量，mask代表掩码张量\n",
    "        # 首先判断是否使用掩码张量\n",
    "        if mask is not None:\n",
    "            # 使用squeeze将掩码张量进行围堵扩充，代表多头的第n个头\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 得到batch_size\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 首先使用 zip 将网络能和输入数据连接在一起，模型的输出 利用 view 和 transpose 进行维度和形状的\n",
    "        query, key, value = \\\n",
    "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)\n",
    "             for model, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 将每个头的输出 传入到注意力层\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.drop)\n",
    "\n",
    "        # 得到每个头的计算结果，每个output都是4维的张量，需要进行维度转换\n",
    "        # 前面已经将transpose（1， 2）\n",
    "        # 注意，先transpose 然后 contiguous，否则无法使用view\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "\n",
    "        # 最后将x输入到线性层的最后一个线性层中进行处理，得到最终的多头注意力结构输出\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "# 实例化若干个参数\n",
    "head = 8\n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# 若干输入参数的初始化\n",
    "query = key = value = pe_result\n",
    "\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "mha = MultiHeadAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)\n",
    "\n",
    "# print(mha_result)\n",
    "# print(mha_result.shape)\n",
    "\n",
    "\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        # d_model : 代表词嵌入的维度，同时也是两个线性层的输入维度和输出维度\n",
    "        # d_ff : 代表第一个线性层的输出维度，和第二个线性层的输入维度\n",
    "        # dropout : 经过Dropout层处理时，随机置零\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        # 定义两层全连接的线性层\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: 来自上一层的输出\n",
    "        # 首先将x送入第一个线性网络，然后relu  然后dropout\n",
    "        # 然后送入第二个线性层\n",
    "        return self.w2(self.dropout(F.relu((self.w1(x)))))\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# 这个是上一层的输出，作为前馈连接的输入\n",
    "x = mha_result\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "ff_result = ff(x)\n",
    "# print(ff_result)\n",
    "# print(ff_result.shape)\n",
    "\n",
    "\n",
    "# 构架规范化层的类\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        # features : 代表词嵌入的维度\n",
    "        # eps ：一个很小的数，防止在规范化公式 除以0\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 初始化两个参数张量 a2 b 2 用于对结果作规范化 操作计算\n",
    "        # 用nn.Parameter  封装，代表他们也是模型中的参数，也要随着模型计算而计算\n",
    "        self.a2 = nn.Parameter(torch.ones(features))\n",
    "        self.b2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps  # 传入到模型中去\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 是上一层网络的输出 （两层的前馈全连接层）\n",
    "        # 首先对x进行 最后一个维度上的求均值操作，同时要求保持输出维度和输入维度一致\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # 接着对x最后一个维度上求标准差的操作，同时要求保持输出维度和输入维度一制\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 按照规范化公式进行计算并返回\n",
    "        return self.a2 * (x-mean) / (std + self.eps) + self.b2\n",
    "\n",
    "\n",
    "features = d_model = 512\n",
    "eps = 1e-6\n",
    "\n",
    "x = ff_result\n",
    "ln = LayerNorm(features, eps)\n",
    "ln_result = ln(x)\n",
    "print(ln_result)\n",
    "print(ln_result.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4029,  2.0160, -2.5725,  ..., -0.0695,  0.3837, -0.7650],\n",
      "         [-1.8618,  0.1176,  1.5201,  ..., -0.6094,  1.7964, -0.4402],\n",
      "         [-1.4517, -0.7813,  0.3225,  ..., -0.9868, -0.0330,  0.4345],\n",
      "         [-0.2051, -1.0454, -0.0742,  ...,  1.5572, -0.1397, -0.9129]],\n",
      "\n",
      "        [[-0.7303,  0.1544,  1.1549,  ..., -1.3101,  0.7498, -0.9943],\n",
      "         [-1.1328,  0.8129,  0.3164,  ...,  1.0723, -0.2347,  0.9507],\n",
      "         [-0.0599, -0.1497,  0.7505,  ..., -1.8553,  1.7566, -0.0384],\n",
      "         [-0.5869,  0.1655, -0.9241,  ..., -1.0236,  0.4136, -0.9395]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# main作用：集成了整个Transformer代码\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "# 构建 Embedding 类来实现文本嵌入层\n",
    "# vocab : 词表的长度， d_model : 词嵌入的维度\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "# 词表： 1000*512， 共是1000个词，每一行是一个词，每个词是一个512d的向量表示\n",
    "vocab = 1000\n",
    "d_model = 512\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "\n",
    "########################################################################################################################\n",
    "# 构建位置编码器的类\n",
    "# d_model : 代表词嵌入的维度\n",
    "# dropout : 代表Dropout层的置零比率\n",
    "# max_len : 代表每个句子的最大长度\n",
    "# 初始化一个位置编码矩阵pe，大小是 max_len * d_model\n",
    "# 初始化一个绝对位置矩阵position, 大小是max_len * 1\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 定义一个变化矩阵，div_term, 跳跃式的初始化\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # 将前面定义的变化矩阵 进行技术，偶数分别赋值\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 用正弦波给偶数部分赋值\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 用余弦波给奇数部分赋值\n",
    "\n",
    "        # 将二维张量，扩充为三维张量\n",
    "        pe = pe.unsqueeze(0)  # 1 * max_len * d_model\n",
    "\n",
    "        # 将位置编码矩阵，注册成模型的buffer，这个buffer不是模型中的参数，不跟随优化器同步更新\n",
    "        # 注册成buffer后，就可以在模型保存后 重新加载的时候，将这个位置编码器和模型参数\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 代表文本序列的词嵌入表示\n",
    "        # 首先明确pe的编码太长了，将第二个维度，就是max_len对应的维度，缩小成x的句子的同等的长度\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)  # 表示位置编码是不参与更新的\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 60\n",
    "vocab = 1000\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "x = embr  # shape: [2, 4, 512]\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "# print(pe_result)\n",
    "\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # query, key, value : 代表注意力的三个输入张量\n",
    "    # mask : 掩码张量\n",
    "    # dropout : 传入Dropout实例化对象\n",
    "    # 首先，将query的最后一个维度提取出来，代表的是词嵌入的维度\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 按照注意力计算公式，将query和key 的转置进行矩阵乘法，然后除以缩放系数\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # print(\"..\", scores.shape)\n",
    "    # 判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 利用masked_fill 方法，将掩码张量和0进行位置的意义比较，如果等于0，就替换成 -1e9\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # scores的最后一个维度上进行 softmax\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 判断是否使用dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后一步完成p_attm 和 value 的乘法，并返回query的注意力表示\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "# print('attn', attn)\n",
    "# print('attn.shape', attn.shape)\n",
    "# print(\"p_attn\", p_attn)\n",
    "# print(p_attn.shape)\n",
    "\n",
    "\n",
    "# 实现克隆函数，因为在多头注意力机制下，要用到多个结果相同的线性层\n",
    "# 需要使用clone 函数u，将他们统一 初始化到一个网络层列表对象中\n",
    "def clones(module, N):\n",
    "    # module : 代表要克隆的目标网络层\n",
    "    # N : 将module几个\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "# 实现多头注意力机制的类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, dropout=0.1):\n",
    "        # head : 代表几个头的函数\n",
    "        # embedding_dim ： 代表词嵌入的维度\n",
    "        # dropout\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # 强调：多头的数量head 需要整除 词嵌入的维度 embedding_dim\n",
    "        assert embedding_dim % head == 0\n",
    "\n",
    "        # 得到每个头，所获得 的词向量的维度\n",
    "        self.d_k = embedding_dim // head\n",
    "        self.head = head\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 获得线性层，需要获得4个，分别是Q K V 以及最终输出的线性层\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "\n",
    "        # 初始化注意力张量\n",
    "        self.attn = None\n",
    "\n",
    "        # 初始化dropout对象\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query,key,value 是注意力机制的三个输入张量，mask代表掩码张量\n",
    "        # 首先判断是否使用掩码张量\n",
    "        if mask is not None:\n",
    "            # 使用squeeze将掩码张量进行围堵扩充，代表多头的第n个头\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 得到batch_size\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 首先使用 zip 将网络能和输入数据连接在一起，模型的输出 利用 view 和 transpose 进行维度和形状的\n",
    "        query, key, value = \\\n",
    "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)\n",
    "             for model, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 将每个头的输出 传入到注意力层\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.drop)\n",
    "\n",
    "        # 得到每个头的计算结果，每个output都是4维的张量，需要进行维度转换\n",
    "        # 前面已经将transpose（1， 2）\n",
    "        # 注意，先transpose 然后 contiguous，否则无法使用view\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "\n",
    "        # 最后将x输入到线性层的最后一个线性层中进行处理，得到最终的多头注意力结构输出\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "# 实例化若干个参数\n",
    "head = 8\n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# 若干输入参数的初始化\n",
    "query = key = value = pe_result\n",
    "\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "mha = MultiHeadAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)\n",
    "\n",
    "# print(mha_result)\n",
    "# print(mha_result.shape)\n",
    "\n",
    "\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        # d_model : 代表词嵌入的维度，同时也是两个线性层的输入维度和输出维度\n",
    "        # d_ff : 代表第一个线性层的输出维度，和第二个线性层的输入维度\n",
    "        # dropout : 经过Dropout层处理时，随机置零\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        # 定义两层全连接的线性层\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: 来自上一层的输出\n",
    "        # 首先将x送入第一个线性网络，然后relu  然后dropout\n",
    "        # 然后送入第二个线性层\n",
    "        return self.w2(self.dropout(F.relu((self.w1(x)))))\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# 这个是上一层的输出，作为前馈连接的输入\n",
    "x = mha_result\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "ff_result = ff(x)\n",
    "# print(ff_result)\n",
    "# print(ff_result.shape)\n",
    "\n",
    "\n",
    "# 构架规范化层的类\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        # features : 代表词嵌入的维度\n",
    "        # eps ：一个很小的数，防止在规范化公式 除以0\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 初始化两个参数张量 a2 b 2 用于对结果作规范化 操作计算\n",
    "        # 用nn.Parameter  封装，代表他们也是模型中的参数，也要随着模型计算而计算\n",
    "        self.a2 = nn.Parameter(torch.ones(features))\n",
    "        self.b2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps  # 传入到模型中去\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 是上一层网络的输出 （两层的前馈全连接层）\n",
    "        # 首先对x进行 最后一个维度上的求均值操作，同时要求保持输出维度和输入维度一致\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # 接着对x最后一个维度上求标准差的操作，同时要求保持输出维度和输入维度一制\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 按照规范化公式进行计算并返回\n",
    "        return self.a2 * (x-mean) / (std + self.eps) + self.b2\n",
    "\n",
    "\n",
    "features = d_model = 512\n",
    "eps = 1e-6\n",
    "\n",
    "x = ff_result\n",
    "ln = LayerNorm(features, eps)\n",
    "ln_result = ln(x)\n",
    "# print(ln_result)\n",
    "# print(ln_result.shape)\n",
    "\n",
    "# 构建子层连接结构的类\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        # size 是词嵌入的维度\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 实例化一个规范化层的对象\n",
    "        self.norm = LayerNorm(size)\\\n",
    "        # 实例化一个dropout对象\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # : x代表上一层传入的张量\n",
    "        # sublayer ： 代表子层连接中 子层函数\n",
    "        # 首先将x进行规范化，送入子层函数，然后dropout， 最后残差连接\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "dropout = 0.2\n",
    "\n",
    "x = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "# 子层函数采用的是多头注意力机制\n",
    "self_attn = MultiHeadAttention(head, d_model)\n",
    "\n",
    "sublayer = lambda x: self_attn(x, x, x, mask)\n",
    "\n",
    "sc = SublayerConnection(size, dropout)\n",
    "sc_result = sc(x, sublayer)\n",
    "# print(sc_result)\n",
    "# print(sc_result.shape)\n",
    "\n",
    "\n",
    "# 构建编码器层的类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        # size : 代表词嵌入的维度\n",
    "        # self_attn : 代表传入的多头自注意力子层的实例化对象\n",
    "        # feed_forward : 代表前馈全连接层实例化对象\n",
    "        # dropout : 进行dropout置零比率\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # 将两个实例化对象和参数传入类中\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.size = size\n",
    "\n",
    "        # 编码器层中，有两个子层连接结构，需要clones函数进行操作\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: 代表上一层传入的张量（位置编码\n",
    "        # mask  ： 代表掩码张量\n",
    "        # 首先让 x 经过第一个子层连接结构，内部包含多头自注意力机制子层\n",
    "        # 再让张量经过第二个子层连接结构，其中包含前馈全连接网络\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "x = pe_result\n",
    "dropout = 0.2\n",
    "self_attn = MultiHeadAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "el = EncoderLayer(size, self_attn, ff, dropout)\n",
    "el_result = el(x, mask)\n",
    "# print(el_result)\n",
    "# print(el_result.shape)\n",
    "\n",
    "\n",
    "# 构建编码器类 Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        # layer : 代表上一节编写的 编码器层\n",
    "        # N ： 代表 编码器中需要 几个编码器层（layer）\n",
    "        super(Encoder, self).__init__()\n",
    "        # 首先使用 clones  函数 克隆 N 个编码器层  防止在self.layer中\n",
    "        self.layers = clones(layer, N)\n",
    "        # 初始化一个规范化层，作用在编码器后面\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 代表上一层输出的张量\n",
    "        # mask 是掩码张量\n",
    "        # 让x 依次经过N个编码器层的处理；最后再经过规范化层就可以输出了\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "dropout = 0.2\n",
    "layer = EncoderLayer(size, c(attn), c(ff), dropout)\n",
    "N = 8\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "en = Encoder(layer, N)\n",
    "en_result = en(x, mask)\n",
    "# print(en_result)\n",
    "# print(en_result.shape)\n",
    "\n",
    "\n",
    "# 构建解码器层类\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        # size ： 代表词嵌入的维度\n",
    "        # attn ：多头自注意力机制对象\n",
    "        # src_attn : 常规的注意力机制对象\n",
    "        # feed_forweawrd : 前馈全连接层\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 使用clones函数， 克隆3个子层连接对象\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        # x: 上一层输入的张量\n",
    "        # memory ： 代表编码器的语义存储张量\n",
    "        # source_mask : 源数据的掩码张量\n",
    "        # target_mask : 目标数据的掩码张量\n",
    "        m = memory\n",
    "\n",
    "        # 第一步，让x 进入第一个子层，（多头自注意力子层\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
    "\n",
    "        # 第二步，让x 进入第二个子层，（常规注意力子层\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, source_mask))\n",
    "\n",
    "        # 第三步，让x进入第三个子层，前馈全连接层\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# 这里就没有区分多头自注意力和常规注意力机制了\n",
    "self_attn = src_attn = MultiHeadAttention(head, d_model, dropout)\n",
    "ff  = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)\n",
    "dl_result = dl(x, memory, source_mask, target_mask)\n",
    "# print(dl_result)\n",
    "# print(dl_result.shape)\n",
    "\n",
    "\n",
    "# 构建解码器类\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        # layer : 代表解码器层 的对象\n",
    "        # N : 代表将layer进行几层的拷贝\n",
    "        super(Decoder, self).__init__()\n",
    "        # 利用clones函数克隆N个layer\n",
    "        self.layers = clones(layer, N)\n",
    "        # 实例化一个规范化层\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        # x: 代表目标数数的嵌入表示\n",
    "        # memory : 代表编码器的输出张量\n",
    "        # source_mask ： 源数据的掩码张量\n",
    "        # target_mask: 目标数据的掩码张量\n",
    "        # x经过所有的编码器层，最后通过规范化层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, source_mask, target_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
    "\n",
    "N = 8\n",
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "de = Decoder(layer, N)\n",
    "de_result = de(x, memory, source_mask, target_mask)\n",
    "print(de_result)\n",
    "print(de_result.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.8956, -6.5986, -8.4811,  ..., -7.2004, -6.7450, -6.9793],\n",
      "         [-7.0791, -5.8751, -8.2197,  ..., -6.8843, -7.3700, -6.7006],\n",
      "         [-6.8995, -6.8604, -7.5096,  ..., -6.8115, -6.6190, -7.3020],\n",
      "         [-7.4304, -7.7939, -7.3737,  ..., -7.5600, -7.7729, -7.5911]],\n",
      "\n",
      "        [[-7.0649, -6.2719, -6.9616,  ..., -5.7521, -6.9926, -7.0587],\n",
      "         [-7.0456, -7.1196, -6.3995,  ..., -6.9879, -6.6631, -6.9461],\n",
      "         [-6.1350, -7.3294, -6.2949,  ..., -6.6793, -6.9646, -7.6704],\n",
      "         [-6.4270, -6.7152, -7.8952,  ..., -7.8854, -6.6828, -7.8045]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([2, 4, 1000])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# main作用：集成了整个Transformer代码\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "# 构建 Embedding 类来实现文本嵌入层\n",
    "# vocab : 词表的长度， d_model : 词嵌入的维度\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "# 词表： 1000*512， 共是1000个词，每一行是一个词，每个词是一个512d的向量表示\n",
    "vocab = 1000\n",
    "d_model = 512\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "\n",
    "########################################################################################################################\n",
    "# 构建位置编码器的类\n",
    "# d_model : 代表词嵌入的维度\n",
    "# dropout : 代表Dropout层的置零比率\n",
    "# max_len : 代表每个句子的最大长度\n",
    "# 初始化一个位置编码矩阵pe，大小是 max_len * d_model\n",
    "# 初始化一个绝对位置矩阵position, 大小是max_len * 1\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 定义一个变化矩阵，div_term, 跳跃式的初始化\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # 将前面定义的变化矩阵 进行技术，偶数分别赋值\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 用正弦波给偶数部分赋值\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 用余弦波给奇数部分赋值\n",
    "\n",
    "        # 将二维张量，扩充为三维张量\n",
    "        pe = pe.unsqueeze(0)  # 1 * max_len * d_model\n",
    "\n",
    "        # 将位置编码矩阵，注册成模型的buffer，这个buffer不是模型中的参数，不跟随优化器同步更新\n",
    "        # 注册成buffer后，就可以在模型保存后 重新加载的时候，将这个位置编码器和模型参数\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 代表文本序列的词嵌入表示\n",
    "        # 首先明确pe的编码太长了，将第二个维度，就是max_len对应的维度，缩小成x的句子的同等的长度\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)  # 表示位置编码是不参与更新的\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 60\n",
    "vocab = 1000\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "emb = Embedding(vocab, d_model)\n",
    "embr = emb(x)\n",
    "x = embr  # shape: [2, 4, 512]\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "# print(pe_result)\n",
    "\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # query, key, value : 代表注意力的三个输入张量\n",
    "    # mask : 掩码张量\n",
    "    # dropout : 传入Dropout实例化对象\n",
    "    # 首先，将query的最后一个维度提取出来，代表的是词嵌入的维度\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 按照注意力计算公式，将query和key 的转置进行矩阵乘法，然后除以缩放系数\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # print(\"..\", scores.shape)\n",
    "    # 判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 利用masked_fill 方法，将掩码张量和0进行位置的意义比较，如果等于0，就替换成 -1e9\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # scores的最后一个维度上进行 softmax\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 判断是否使用dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后一步完成p_attm 和 value 的乘法，并返回query的注意力表示\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "# print('attn', attn)\n",
    "# print('attn.shape', attn.shape)\n",
    "# print(\"p_attn\", p_attn)\n",
    "# print(p_attn.shape)\n",
    "\n",
    "\n",
    "# 实现克隆函数，因为在多头注意力机制下，要用到多个结果相同的线性层\n",
    "# 需要使用clone 函数u，将他们统一 初始化到一个网络层列表对象中\n",
    "def clones(module, N):\n",
    "    # module : 代表要克隆的目标网络层\n",
    "    # N : 将module几个\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "# 实现多头注意力机制的类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, dropout=0.1):\n",
    "        # head : 代表几个头的函数\n",
    "        # embedding_dim ： 代表词嵌入的维度\n",
    "        # dropout\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # 强调：多头的数量head 需要整除 词嵌入的维度 embedding_dim\n",
    "        assert embedding_dim % head == 0\n",
    "\n",
    "        # 得到每个头，所获得 的词向量的维度\n",
    "        self.d_k = embedding_dim // head\n",
    "        self.head = head\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 获得线性层，需要获得4个，分别是Q K V 以及最终输出的线性层\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "\n",
    "        # 初始化注意力张量\n",
    "        self.attn = None\n",
    "\n",
    "        # 初始化dropout对象\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query,key,value 是注意力机制的三个输入张量，mask代表掩码张量\n",
    "        # 首先判断是否使用掩码张量\n",
    "        if mask is not None:\n",
    "            # 使用squeeze将掩码张量进行围堵扩充，代表多头的第n个头\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 得到batch_size\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 首先使用 zip 将网络能和输入数据连接在一起，模型的输出 利用 view 和 transpose 进行维度和形状的\n",
    "        query, key, value = \\\n",
    "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)\n",
    "             for model, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 将每个头的输出 传入到注意力层\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.drop)\n",
    "\n",
    "        # 得到每个头的计算结果，每个output都是4维的张量，需要进行维度转换\n",
    "        # 前面已经将transpose（1， 2）\n",
    "        # 注意，先transpose 然后 contiguous，否则无法使用view\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "\n",
    "        # 最后将x输入到线性层的最后一个线性层中进行处理，得到最终的多头注意力结构输出\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "# 实例化若干个参数\n",
    "head = 8\n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# 若干输入参数的初始化\n",
    "query = key = value = pe_result\n",
    "\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "mha = MultiHeadAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)\n",
    "\n",
    "# print(mha_result)\n",
    "# print(mha_result.shape)\n",
    "\n",
    "\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        # d_model : 代表词嵌入的维度，同时也是两个线性层的输入维度和输出维度\n",
    "        # d_ff : 代表第一个线性层的输出维度，和第二个线性层的输入维度\n",
    "        # dropout : 经过Dropout层处理时，随机置零\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        # 定义两层全连接的线性层\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: 来自上一层的输出\n",
    "        # 首先将x送入第一个线性网络，然后relu  然后dropout\n",
    "        # 然后送入第二个线性层\n",
    "        return self.w2(self.dropout(F.relu((self.w1(x)))))\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# 这个是上一层的输出，作为前馈连接的输入\n",
    "x = mha_result\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "ff_result = ff(x)\n",
    "# print(ff_result)\n",
    "# print(ff_result.shape)\n",
    "\n",
    "\n",
    "# 构架规范化层的类\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        # features : 代表词嵌入的维度\n",
    "        # eps ：一个很小的数，防止在规范化公式 除以0\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 初始化两个参数张量 a2 b 2 用于对结果作规范化 操作计算\n",
    "        # 用nn.Parameter  封装，代表他们也是模型中的参数，也要随着模型计算而计算\n",
    "        self.a2 = nn.Parameter(torch.ones(features))\n",
    "        self.b2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps  # 传入到模型中去\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 是上一层网络的输出 （两层的前馈全连接层）\n",
    "        # 首先对x进行 最后一个维度上的求均值操作，同时要求保持输出维度和输入维度一致\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # 接着对x最后一个维度上求标准差的操作，同时要求保持输出维度和输入维度一制\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 按照规范化公式进行计算并返回\n",
    "        return self.a2 * (x-mean) / (std + self.eps) + self.b2\n",
    "\n",
    "\n",
    "features = d_model = 512\n",
    "eps = 1e-6\n",
    "\n",
    "x = ff_result\n",
    "ln = LayerNorm(features, eps)\n",
    "ln_result = ln(x)\n",
    "# print(ln_result)\n",
    "# print(ln_result.shape)\n",
    "\n",
    "# 构建子层连接结构的类\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        # size 是词嵌入的维度\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 实例化一个规范化层的对象\n",
    "        self.norm = LayerNorm(size)\\\n",
    "        # 实例化一个dropout对象\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # : x代表上一层传入的张量\n",
    "        # sublayer ： 代表子层连接中 子层函数\n",
    "        # 首先将x进行规范化，送入子层函数，然后dropout， 最后残差连接\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "dropout = 0.2\n",
    "\n",
    "x = pe_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "# 子层函数采用的是多头注意力机制\n",
    "self_attn = MultiHeadAttention(head, d_model)\n",
    "\n",
    "sublayer = lambda x: self_attn(x, x, x, mask)\n",
    "\n",
    "sc = SublayerConnection(size, dropout)\n",
    "sc_result = sc(x, sublayer)\n",
    "# print(sc_result)\n",
    "# print(sc_result.shape)\n",
    "\n",
    "\n",
    "# 构建编码器层的类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        # size : 代表词嵌入的维度\n",
    "        # self_attn : 代表传入的多头自注意力子层的实例化对象\n",
    "        # feed_forward : 代表前馈全连接层实例化对象\n",
    "        # dropout : 进行dropout置零比率\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # 将两个实例化对象和参数传入类中\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.size = size\n",
    "\n",
    "        # 编码器层中，有两个子层连接结构，需要clones函数进行操作\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: 代表上一层传入的张量（位置编码\n",
    "        # mask  ： 代表掩码张量\n",
    "        # 首先让 x 经过第一个子层连接结构，内部包含多头自注意力机制子层\n",
    "        # 再让张量经过第二个子层连接结构，其中包含前馈全连接网络\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "x = pe_result\n",
    "dropout = 0.2\n",
    "self_attn = MultiHeadAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "el = EncoderLayer(size, self_attn, ff, dropout)\n",
    "el_result = el(x, mask)\n",
    "# print(el_result)\n",
    "# print(el_result.shape)\n",
    "\n",
    "\n",
    "# 构建编码器类 Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        # layer : 代表上一节编写的 编码器层\n",
    "        # N ： 代表 编码器中需要 几个编码器层（layer）\n",
    "        super(Encoder, self).__init__()\n",
    "        # 首先使用 clones  函数 克隆 N 个编码器层  防止在self.layer中\n",
    "        self.layers = clones(layer, N)\n",
    "        # 初始化一个规范化层，作用在编码器后面\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 代表上一层输出的张量\n",
    "        # mask 是掩码张量\n",
    "        # 让x 依次经过N个编码器层的处理；最后再经过规范化层就可以输出了\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "dropout = 0.2\n",
    "layer = EncoderLayer(size, c(attn), c(ff), dropout)\n",
    "N = 8\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "en = Encoder(layer, N)\n",
    "en_result = en(x, mask)\n",
    "# print(en_result)\n",
    "# print(en_result.shape)\n",
    "\n",
    "\n",
    "# 构建解码器层类\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        # size ： 代表词嵌入的维度\n",
    "        # attn ：多头自注意力机制对象\n",
    "        # src_attn : 常规的注意力机制对象\n",
    "        # feed_forweawrd : 前馈全连接层\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 使用clones函数， 克隆3个子层连接对象\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        # x: 上一层输入的张量\n",
    "        # memory ： 代表编码器的语义存储张量\n",
    "        # source_mask : 源数据的掩码张量\n",
    "        # target_mask : 目标数据的掩码张量\n",
    "        m = memory\n",
    "\n",
    "        # 第一步，让x 进入第一个子层，（多头自注意力子层\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
    "\n",
    "        # 第二步，让x 进入第二个子层，（常规注意力子层\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, source_mask))\n",
    "\n",
    "        # 第三步，让x进入第三个子层，前馈全连接层\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# 这里就没有区分多头自注意力和常规注意力机制了\n",
    "self_attn = src_attn = MultiHeadAttention(head, d_model, dropout)\n",
    "ff  = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)\n",
    "dl_result = dl(x, memory, source_mask, target_mask)\n",
    "# print(dl_result)\n",
    "# print(dl_result.shape)\n",
    "\n",
    "\n",
    "# 构建解码器类\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        # layer : 代表解码器层 的对象\n",
    "        # N : 代表将layer进行几层的拷贝\n",
    "        super(Decoder, self).__init__()\n",
    "        # 利用clones函数克隆N个layer\n",
    "        self.layers = clones(layer, N)\n",
    "        # 实例化一个规范化层\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        # x: 代表目标数数的嵌入表示\n",
    "        # memory : 代表编码器的输出张量\n",
    "        # source_mask ： 源数据的掩码张量\n",
    "        # target_mask: 目标数据的掩码张量\n",
    "        # x经过所有的编码器层，最后通过规范化层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, source_mask, target_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "size = d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
    "\n",
    "N = 8\n",
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "de = Decoder(layer, N)\n",
    "de_result = de(x, memory, source_mask, target_mask)\n",
    "# print(de_result)\n",
    "# print(de_result.shape)\n",
    "\n",
    "\n",
    "# 构建Generate类\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        # d_model : 代表词嵌入的维度\n",
    "        # vovab_size : 代表词表的总大小\n",
    "        super(Generator, self).__init__()\n",
    "        # 定义一个线性层，完成网络输出维度的变换\n",
    "        self.project = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x ： 上一层的输出张量\n",
    "        # 首先将x送入线性层中，让其经历softmax处理\n",
    "        return F.log_softmax(self.project(x), dim=-1)\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "vocab_size = 1000\n",
    "x = de_result\n",
    "\n",
    "gen = Generator(d_model, vocab_size)\n",
    "gen_result = gen(x)\n",
    "print(gen_result)\n",
    "print(gen_result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建编码器- 解码器结构类\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
    "        # encoder : 编码器对象\n",
    "        # decoder : 解码器对象\n",
    "        # source_embed : 源数据的嵌入函数\n",
    "        # target_embed : 目标数据的嵌入函数\n",
    "        # generator : 输出部分类别生成器 对象\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_emned = source_embed\n",
    "        self.tgt_embed = target_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        # source : 代表源数据\n",
    "        # target : 代表目标数据\n",
    "        # source_mask : 代表源数据的掩码张量\n",
    "        # target_mask : 代表目标数据的掩码张量\n",
    "        return self.decode(self.encode(source, source_mask), source_mask,\n",
    "                           target, target_mask)\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        return self.encoder(self.src_emned(source), source_mask)\n",
    "\n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        # memory : 代表经历编码器编码后的输出张量\n",
    "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2461, -1.9424, -0.2390,  ...,  0.3894, -2.0771,  0.4588],\n",
      "         [ 2.1275, -1.4796, -0.1383,  ...,  0.5331, -1.5249,  0.6149],\n",
      "         [ 0.2116, -0.6087, -0.3678,  ..., -0.2838, -2.2089,  0.3160],\n",
      "         [ 1.4840, -1.8253, -0.7596,  ..., -0.3792, -1.7015, -0.0275]],\n",
      "\n",
      "        [[ 0.4927,  0.2528,  0.9127,  ..., -0.1492, -1.7515, -0.5555],\n",
      "         [ 0.6065, -0.7407, -0.4137,  ..., -0.4184, -0.7686, -0.3387],\n",
      "         [ 0.1525, -0.6770, -0.3392,  ...,  0.0147, -1.3579, -1.0997],\n",
      "         [ 0.3306,  0.6021, -0.1215,  ..., -0.0857, -1.4704, -1.1116]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "# 构建编码器- 解码器结构类\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
    "        # encoder : 编码器对象\n",
    "        # decoder : 解码器对象\n",
    "        # source_embed : 源数据的嵌入函数\n",
    "        # target_embed : 目标数据的嵌入函数\n",
    "        # generator : 输出部分类别生成器 对象\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_emned = source_embed\n",
    "        self.tgt_embed = target_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        # source : 代表源数据\n",
    "        # target : 代表目标数据\n",
    "        # source_mask : 代表源数据的掩码张量\n",
    "        # target_mask : 代表目标数据的掩码张量\n",
    "        return self.decode(self.encode(source, source_mask), source_mask,\n",
    "                           target, target_mask)\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        return self.encoder(self.src_emned(source), source_mask)\n",
    "\n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        # memory : 代表经历编码器编码后的输出张量\n",
    "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n",
    "\n",
    "\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "encoder = en\n",
    "decoder = de\n",
    "source_embed = nn.Embedding(vocab_size, d_model)\n",
    "target_embed = nn.Embedding(vocab_size, d_model)\n",
    "generator = gen\n",
    "\n",
    "source = target = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "source_mask = target_mask = Variable(torch.zeros(2, 4, 4))\n",
    "\n",
    "ed = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)\n",
    "ed_result = ed(source, target, source_mask, target_mask)\n",
    "print(ed_result)\n",
    "print(ed_result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(source_vocab, target_vocab, N=6, d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
    "    # source_vocab : 源数据的词汇总数\n",
    "    # target_vocab : 目标数据的词汇总数\n",
    "    # N : 编码器和解码器堆叠的层数\n",
    "    # d_model : 词嵌入的维度\n",
    "    # d_ff : 前馈全连接层中变换矩阵的维度\n",
    "    # head : 多头注意力机制头数\n",
    "    # dropout :  置零比率\n",
    "    c = copy.deepcopy\n",
    "\n",
    "    # 实例化一个多头注意力类\n",
    "    attn = MultiHeadAttention(head, d_model)\n",
    "\n",
    "    # 实例化一个前馈全连接层的网络对象\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    # 实例化一个位置编码器\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "    # 实例化模型model, 利用的是Encoder和Decoder类\n",
    "    # 编码器的结构里面有2个子层，attention层 和 前馈全连接层\n",
    "    # 解码器的结构中有3 个子层，两个attention 层和前馈全连接层\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embedding(d_model, source_vocab), c(position)),\n",
    "        nn.Sequential(Embedding(d_model, target_vocab), c(position)),\\\n",
    "        Generator(d_model, target_vocab)\n",
    "    )\n",
    "\n",
    "    # 初始化整个模型中的参数，如果参数的维度大于1，将矩阵初始化成一个服从均匀分布的矩阵\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据生成器\n",
    "def data_generator(V, batch_size, num_batch):\n",
    "    # 随机生成数据的最大值=1, 假设V=101， 生成的数据是 1-100\n",
    "    # batch_size : 每次喂给模型的样本数量\n",
    "    # num_batch ；一共喂模型 多少轮\n",
    "    for i in range(num_batch):\n",
    "        # 使用numpy中的random.randint()来随机生产 [1, V)\n",
    "        # 分布的形状是 (batch, 10)\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch_size, 10)))\n",
    "\n",
    "        # 将数据的一列全部设置为1， 作为起始标志\n",
    "        data[:, 0] = 1\n",
    "\n",
    "        # 因为是copy任务，所以源数据和目标数据完全一致\n",
    "        # 设置参数 requires_grad=False，样本的参数不需要参与梯度的计算\n",
    "        source = Variable(data, requires_grad=False)\n",
    "        target = Variable(data, requires_grad=False)\n",
    "\n",
    "        yield Batch(source, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyitcast.transformer_utils import Batch\n",
    "from pyitcast.transformer_utils import get_std_opt\n",
    "from pyitcast.transformer_utils import LabelSmoothing\n",
    "from pyitcast.transformer_utils import SimpleLossCompute\n",
    "from pyitcast.transformer_utils import run_epoch\n",
    "from pyitcast.transformer_utils import greedy_decode\n",
    "\n",
    "V = 11\n",
    "batch_size = 20\n",
    "num_batch = 30\n",
    "\n",
    "# 首先使用make_model()函数 生成模型的实例化对象\n",
    "model = make_model(V, V, N=2)\n",
    "\n",
    "# 使用工具包 get_std-opt 获得模型的优化器\n",
    "model_optimizer = get_std_opt(model)\n",
    "\n",
    "# 使用labelSmoothing 获得标签平滑对象\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "# 使用工具包 SimpleLossCompute 获得利用标签平滑的结果，得到损失计算方法\n",
    "loss = SimpleLossCompute(model.generator, criterion, model_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
