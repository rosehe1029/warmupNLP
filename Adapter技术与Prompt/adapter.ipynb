{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapter-Tuning与Prompt-Tuning的区别\n",
    "\n",
    "Adapter是在Transformer中增加了额外的网络层。对于连续型的Prompt，以Prefix-Tuning为例，是在Transformer的每层增加了额外的prefix vector，利用attention机制来发挥作用。用不太严谨的话来说，Adapter-Tuning像是纵向的参数扩展，而Prefix-tuning像是横向的参数扩展。相对而言，Prompt-Tuning训练的参数量更小。\n",
    "\n",
    "源引Prefix-Tuning中的一段话：\n",
    "\n",
    "    Recall that prefix-tuning keeps the LM intact and uses the prefix and the pretrained attention blocks to affect the subsequent activations; adapter-tuning inserts trainable modules between LM layers, which directly add residual vectors to the activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先adapter方法的原理并不复杂，它是通过在原始的预训练模型中的每个transformer block中加入一些参数可训练的模块实现的。\n",
    "\n",
    "\n",
    "假设原始的预训练模型的参数为ω，加入的adapter 参数为υ，在针对不同下游任务进行调整时，只需要将预训练参数固定住，只针对adapter参数υ进行训练。\n",
    "\n",
    "\n",
    "通常情况下，参数量υ<<ω, 因此在对多个下游任务调整时，只需要调整极小数量的参数，大大的提高了预训练模型的扩展性和实用性。\n",
    "\n",
    "\n",
    "对于adapter模块的网络组成，不同文章中针对不同任务略有不同。但是比较一致的结论是，bottleneck形式的两层全连接神经网络就已经可以满足要求。\n",
    "\n",
    "\n",
    "在Houlsby[1]的文章中，每个transformer 层中有两个adapter模块，在每个adapter模块中，先将经过多头注意力和前馈层输出的output做一个降维的映射。\n",
    "\n",
    "\n",
    "经过一个非线性激活层后，再将特征矢量映射回原始的维度。在下游训练任务中，只更新adapter模块和layer Norm 层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于预训练模型的全参数微调，Adapter 方法的优势十分明显：\n",
    "\n",
    "\n",
    "（1）针对不同下游任务可保持预训练模型不变，仅需训练adapter模块的少量参数，训练代价小，可移植性强。\n",
    "\n",
    "\n",
    "（2）对于不同任务的连续学习（continual learning）而言，由于在训练不同任务时只需要训练不同的adapter，其他模型参数保持不变，避免了在学习新任务时对过往任务的遗忘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考：https://github.com/Adapter-Hub/adapter-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
